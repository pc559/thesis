%\input{commands}
\chapter{Introduction to cosmology}\label{chapter:intro_general}
\section{General introduction}\label{sec:general_intro}
%\subsection{The bispectrum}
    \subsection{Fundamentals}
    (A discussion of GR etc.)
\newpage
    Words
\newpage
    \subsection{$\lcdm$}
    (Outlining $\lcdm$.)
\newpage
    Words
\newpage
    Words
\newpage
    Words
\newpage
\section{Initial conditions for $\lcdm$}
    \subsection{Motivations for inflation}
    Before the popsci Big Bang. Horizon problem etc.
\newpage
    Words
\newpage
    Words
\newpage
    \subsection{Criteria for successful inflation}
    Sufficient e-folds etc.
\newpage
    Words
\newpage
\section{Statistical observables}
    \subsection{Checking dice for fairness}
    The technicalities and limitations involved in determining a statistical observable,
    the concept of variance, cosmic variance.
    Ergodicity: ``In an ergodic scenario, the average outcome of the group is the same as the average outcome of the individual over time. An example of an ergodic systems would be the outcomes of a coin toss (heads/tails). If 100 people flip a coin once or 1 person flips a coin 100 times, you get the same outcome.'' from https://taylorpearson.me/ergodicity/


    The prediction of the fundamental quantum theory is a statistical one,
    i.e.\ a prediction of the distribution from which our observation will be drawn.
    As such, we need to talk in terms of estimators, estimating how unlikely it
    would be to see the sky we do, assuming the fundamental theory.
\newpage
    \subsection{Power spectra}
    Define n-point correlations, their Fourier transforms, talk about them as observables.
\newpage
\section{Observational data}
    \subsection{\planck, Simons} 
    High-level descriptions.
\newpage
    \subsection{Future missions}
    High-level descriptions.
\newpage
\section{Outline of thesis}
    \subsection{Goals}
    \begin{enumerate}
        \item Connecting inflation models directly to observations,
            through the bispectrum.
        \item Constraining the parameters of inflation models, not phenomenological templates and $f_{NL}$.
        \item To obtain the full shape information, not point samples or a limit.
        \item Efficient numerics gives access to more accurate, and in some cases new, feature shapes.
    \end{enumerate}
\newpage
    \subsection{Methods}
    \begin{enumerate}
        \item Building separability into the tree-level in-in formalism.
        \item The CMB calculation~\cite{Sohn_2021}: expensive, but need only be done once per primordial basis.
        \item So, we want a basis expansion that converges quickly for a broad range of inflation models.
        \item Convergence on the cube is different to the tetrapyd.
        \item Turns out to be much faster at primordial level than previous numerical methods
            (as it in a sense converges way faster, and as it enables us to use faster numerical methods than otherwise).
    \end{enumerate}
\newpage
    \subsection{Results}
    \begin{enumerate}
        \item First development/implementation of the formalism for calculating the expansion to high orders.
        \item We recognised and described the central issue of the cube vs tetra problem.
        \item Found a basis with broad descriptive power (and other less powerful basis sets).
        \item This allowed the first validation of these methods on features.
        \item Explore and characterise DBI reso model TBC? Validity of approximations, e.g.\ Tanh kink.
        \item Connect to CMB, get constraints TBC?
    \end{enumerate}
