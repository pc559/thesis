%\input{commands}
\chapter{Introduction to cosmology}\label{chapter:intro_general}
\section{General introduction}\label{sec:general_intro}
%\subsection{The bispectrum}
    \subsection{Fundamentals}
    (A discussion of GR etc.)
\newpage
    Words
\newpage
    \subsection{$\lcdm$}
    (Outlining $\lcdm$.)
\newpage
    Words
\newpage
    Words
\newpage
    Words
\newpage
\section{Initial conditions for $\lcdm$}
    \subsection{Motivations for inflation}
    Before the popsci Big Bang. Horizon problem etc.
\newpage
    Words
\newpage
    Words
\newpage
    \subsection{Criteria for successful inflation}
    Sufficient e-folds etc.
\newpage
    Words
\newpage
\section{Statistical observables}
    \subsection{Checking dice for fairness}
    The prediction of the fundamental quantum theory is a statistical one,
    i.e.\ a prediction of the distribution from which our observation will be drawn.
    As such, we need to talk in terms of estimators, estimating how likely it
    would be to see the sky we do, assuming some fundamental theory.


    Our result will be a constraint on an inflationary parameter, stated at e.g.\ $95\%$ confidence.
    For example, if we flipped one fair coin $100$ times, we would expect an even split of heads
    and tails. Should we be suspicious of the fairness of the coin if we instead get $60$ heads
    and $40$ tails? Or $80$ heads and $20$ tails? An event at least as extreme at a $60$-$40$ split
    will occur $3.5\%$ of the time. For a $80$-$20$ split however, the probability of an event that
    extreme is around one part in $10^{10}$, effectively impossible. This means that if we observed
    a trial where a coin came up heads in $80$ out of $100$ cases, we could suspect that out hypothesis
    of fairness was incorrect.
    One can calculate that, for a fair coin, $95\%$ of the time the events will fall within the
    range $[41,59]$. We call this the $95\%$ confidence interval.
    What we wish to do is the opposite, however. Instead of calculating using known probabilities,
    we wish to take a series of observations of some random variable and use these observations
    to estimate the probability distribution that they were drawn from. Given an infinite number
    of observations this task has a straightforward solution, simply plotting the normalised
    histogram of the results. However, in the context of cosmology we will only have a finite amount
    of information available to us, and thus a limit to how well we can ever measure the relevant
    probability distributions. This limitation is known as cosmic variance.



    We quantify this expected scatter around the mean using a quantity known as the standard deviation.
    The standard deviation of a random variable $X$ is the square root of the expected value of the
    squared deviation from the mean $\mu$,
    \begin{align}
        \sigma = \sqrt{E\left[{(X-\mu)}^2\right]}.
    \end{align}
    In our example above of flipping one coin $100$ times, the total number of heads has $\mu=50$
    and $\sigma=5$.

    
    We have only one universe that we can observe, only one draw from the probability distribution
    we are trying to probe. However, there is a lot of information in that one draw.
    \textcolor{red}{Motivate and explain this all better!} This is related to the concept
    of ergodicity. This is the statement that $\left<\cdot\right>$ as an expectation over ensembles
    at a fixed point is the same as the expectation over point for a fixed ensemble.
    This assumes homogeneity, stationarity, and that distant points are uncorrelated.
    %Ergodicity: ``In an ergodic scenario, the average outcome of the group is the same as the average outcome of the individual over time. An example of an ergodic systems would be the outcomes of a coin toss (heads/tails). If 100 people flip a coin once or 1 person flips a coin 100 times, you get the same outcome.'' from https://taylorpearson.me/ergodicity/
    % See also https://nms.kcl.ac.uk/eugene.lim/AdvCos/lecture2.pdf


    We can define the expectation value of a function of a discrete or
    continuous random variable $x$, or a functional $F$ of a field configuration $f(x)$, respectively as
    \begin{align}
        \left<f(x)\right> &= \sum_i x_i P(x_i)\label{expectation_value_discrete}\\
        \left<f(x)\right> &= \int dx~x \rho(x)\label{expectation_value_cont}\\
        \left<F\left[f(x)\right]\right> &= \int \mathcal{D}f~F\left[f(x)\right] P\left[f(x)\right]\label{expectation_value_field}
    \end{align}
    where the sum and integral over $x$ are over the values that $x$ can take,
    and the functional integral over $f$ is over all the field configurations
    that $f(x)$ can take.


    In our example of the $100$ coin flips, we used~\eqref{expectation_value_discrete}
    to calculate the mean and standard deviation. For a continuous variable,
    like the average height of a population or the average temperature in a given room,
    we would use~\eqref{expectation_value_cont}. In this thesis, we will be working with
    the expectation value of field configurations, so we will use~\eqref{expectation_value_field}.
    \textcolor{red}{Talk about quantum to classical transition!}


    The power spectrum and other correlations
    are predicted by inflation. 
\newpage
    \subsection{Power spectra}
    Define n-point correlations, their Fourier transforms, talk about them as observables.
\newpage
\section{Observational data}
    \subsection{\planck, Simons} 
    High-level descriptions.
\newpage
    \subsection{Future missions}
    High-level descriptions.
\newpage
\section{Outline of thesis}
    \subsection{Goals}
    \begin{enumerate}
        \item Connecting inflation models directly to observations,
            through the bispectrum.
        \item Constraining the parameters of inflation models, not phenomenological templates and $f_{NL}$.
        \item To obtain the full shape information, not point samples or a limit.
        \item Efficient numerics gives access to more accurate, and in some cases new, feature shapes.
    \end{enumerate}
\newpage
    \subsection{Methods}
    \begin{enumerate}
        \item Building separability into the tree-level in-in formalism.
        \item The CMB calculation~\cite{Sohn_2021}: expensive, but need only be done once per primordial basis.
        \item So, we want a basis expansion that converges quickly for a broad range of inflation models.
        \item Convergence on the cube is different to the tetrapyd.
        \item Turns out to be much faster at primordial level than previous numerical methods
            (as it in a sense converges way faster, and as it enables us to use faster numerical methods than otherwise).
    \end{enumerate}
\newpage
    \subsection{Results}
    \begin{enumerate}
        \item First development/implementation of the formalism for calculating the expansion to high orders.
        \item We recognised and described the central issue of the cube vs tetra problem.
        \item Found a basis with broad descriptive power (and other less powerful basis sets).
        \item This allowed the first validation of these methods on features.
        \item Explore and characterise DBI reso model TBC? Validity of approximations, e.g.\ Tanh kink.
        \item Connect to CMB, get constraints TBC?
    \end{enumerate}
