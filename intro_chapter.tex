%\input{commands}
\chapter{Introduction I: Cosmology}\label{chapter:intro_general}
\section{The standard cosmological history}\label{sec:general_intro}
%\subsection{The bispectrum}
    \subsection{Fundamentals}
When we look out into the sky, we see light that has traveled for a long time to get to us.
In that light we see images of the universe as it was when the light left its source.
We see the moon as it was a second ago, or the Andromeda Galaxy as it was millions of
years ago. To what extreme can we push this? How far back in time can we see? As we look further
away, and further back in time, we see a universe that looks nothing like the one we know today.
We see a universe that was extremely hot and dense, but also extremely uniform. Today
we have distinct galaxies and the empty spaces in between, but looking far enough back we see a universe
without that structure, instead simply uniformly hot and dense everywhere.


But this uniformity wasn't perfect. There were patches that were very slightly more dense than the average,
and others that were very slightly less dense. These non-uniformities provided the seeds that would form
structure---the over-dense regions had a slightly higher gravitational pull on their surroundings,
and caused the nearby matter to begin to fall in. This process would eventually result in the structure
we see in the universe today: the stars, the galaxies, and all the complexity on our own planet.


The precise details of this story have been fleshed out thanks to the work of many people.
It is incredible that we can say so much about the radically different epochs of the universe that came before our own,
between this hot, dense initial state and today.
More work remains to be done, however---one aspect we can question is the origin of the non-uniformities
that provided the seeds of structure. What was the physics that generated them?
The goal of the work described in this thesis is to develop methods to connect this physics
to the sky we see today; more precisely, we aim to use the light from the early universe (the cosmic microwave
background, the $\cmb$)
to constrain physical theories of the very early universe (the epoch of inflation) using a particular
statistical description, known as the bispectrum.
We aim to mature the connection between models of inflation and their predicted bispectra, so we can better
extract the fundamental physics waiting to be found in the universe's youth.


    We begin with a brief discussion of the fundamentals of cosmology.
    General relativity is the theory which describes how the evolution of the universe depends
    on its matter and energy content. It describes the path of light as a geodesic, a straight line
    in spacetime---with spacetime being described in terms of curved geometry.
    The equations that we derive from this theory (the Einstein equations) are written in terms of a metric,
    which we denote by $g_{\mu\nu}$. This object defines our notion of distance, and thus encodes
    the causal structure of the universe---which events can influence which other events.
    

    Spatially homogeneous and isotropic universes (i.e.\ ones that at some given
    time, look the same everywhere and in every direction)
    define a class of solutions to the Einstein equations.
    The metric of each universe is known as a Friedmann–Lemaître–Robertson–Walker metric
    (or more usually, an FLRW metric). In this context
    the Einstein equations become the Friedmann equations, which we will soon present
    in~\eqref{friedmann_1}.
    But first, we present the the FLRW metric (we will always set the speed of light to unity)
    \begin{align}\label{flrw_metric}
        ds^2 = g_{\mu\nu}dx^{\mu}dx^{\nu} = - dt^2 + a(t)^2 dS_3^2,
    \end{align}
    where $a(t)$ has the interpretation as the scale factor
    which describes the evolution of the universe.
    The line element $ds^2$ then determines the interval between two spacetime points.
    %Or, more intuitively, it determines the proper time measured by an observer who freely falls
    %between two events to be the integral of $d\tau=\sqrt{-g_{\mu\nu}dx^\mu dx^\nu}$.
    The homogeneity, isotropy and curvature of space are encoded in $dS^2_3$,
    but a non-trivial time dependence of $a(t)$ breaks the homogeneity of time.
    We will only consider a flat universe, so $dS_3^2=dx^2+dy^2+dz^2$.


    The curvature of spacetime and its evolution is determined by the matter and energy
    present in the universe.
    Radiation is one of the cosmologically important components of the present universe, in the form of the
    photons in the cosmic microwave background, the $\cmb$.
    Today these photons free-stream through the universe.
    Another component is matter, by which we mean \textit{dark} matter.
    This is invisible to our telescopes (which depend on electromagnetic radiation) but can be mapped by the
    effect its mass has on the curvature of spacetime, which we see through
    effects such as the lensing of passing photons.
    A more familiar component is what cosmologists refer to as baryonic matter, which is
    the protons, electrons, neutrons and all the other particles which make up the visible galaxies, stars
    and humans.
    The last component we will mention is the component which at present makes up the largest contribution to the
    energy budget of the universe, dark energy.
    Dark energy appears to act as a vacuum energy, as a cosmological constant which does not
    dilute as the universe expands. The radiation is denoted by $\gamma$,
    the dark matter by $m$, and the cosmological constant by $\Lambda$.


    The components of the matter and energy content of the universe enter the equations through
    their energy-momentum tensors $T^{\mu\nu}$, which we approximate as that of
    a perfect fluid. Labeling the fluid $X$,
    \begin{align}\label{stress_tensor}
        T_X^{\mu\nu} = (\rho_X+P_X)U_X^\mu U_X^\nu+P_Xg^{\mu\nu},
    \end{align}
    where the index structure ensures the fluid is isotropic in its rest frame.
    $U_X^\mu$ is the four-velocity of the fluid, with $U_X^0=1$ and $U_X^i=0$
    in the rest frame.
    These fluids are completely characterised by the time dependence
    of their rest-frame energy density $\rho_X(t)$ and their rest-frame
    pressure density $P_X(t)$,
    both of which have no dependence on position, to preserve the homogeneity of the solution.
    For the components of the universe that we wish to describe,
    taking the quantity $w_X=\frac{P_X}{\rho_X}$ as constant will be a good description.
    This quantity $w$ takes the values $0$, $1/3$ and $-1$ for matter, radiation
    and dark energy (described by a cosmological constant) respectively.


    \begin{table}[h!]
    \begin{center}
        \begin{tabular}{ c c c c }
            Epoch & $w$ & $a(t)$ & $a(\tau)$ \\ 
            \toprule
            Radiation & $\frac{1}{3}$ & $t^{\frac{1}{2}}$ & $\tau$ \\
            Matter & $0$ & $t^{\frac{2}{3}}$ & $\tau^2$ \\
            $\Lambda$ & $-1$ & $e^{Ht}$ & $-\frac{1}{\tau}$
        \end{tabular}\caption{
            How the scale factor, $a(t)$, evolves in the
            different epochs of the universe.
        }\label{lcdm_dep_table}
    \end{center}
    \end{table}


    Without spatial curvature, the Einstein equations are then
    \begin{align}\label{einstein_equations}
        G_{\mu\nu} = 8\pi G T_{\mu\nu}
    \end{align}
    where $G_{\mu\nu}$ is a complicated function of $g_{\mu\nu}$ which
    will be presented in~\eqref{einstein_tensor}.
    Fortunately, when $g_{\mu\nu}$ takes the form~\eqref{flrw_metric},
    these equations simplify to the promised Friedmann equations
    %\footnote{
    %    In~\eqref{einstein_equations} and~\eqref{friedmann_1} we have temporarily reinstated $8\pi G$.}
    \begin{align}\label{friedmann_1}
        H^2 &= \frac{8\pi G \rho}{3},\\
        \frac{\ddot{a}}{a} &= -\frac{4\pi G}{3}\left(\rho+3P\right),
    \end{align}
    where $H=\frac{\dot{a}}{a}$, and $\rho$ and $P$ are respectively the sum of the
    energy densities and of the pressure densities of the
    components of the universe.
    A dot will refer to a derivative with respect to coordinate time $t$.
    We also have the continuity equation
    \begin{align}\label{continuity_equation}
        \dot{\rho}_X + 3\frac{\dot{a}}{a}\left(\rho_X+P_X\right) &= 0
    \end{align}
    which is satisfied for each component separately, and describes
    the effect the expansion of the universe has on the energy density of the fluid.

    Given the equations of state and the densities of the different
    components of the universe, one can then calculate the time dependence of the
    scale factor $a(t)$ and of the densities $\rho_X$ and $P_X$ of each component $X$.
    This gives us the homogeneous background evolution of the universe---evolution
    which has observable effects in physical phenomena
    such as red-shifted photons arriving from distant galaxies.
    As we have mentioned, such
    photons travel on geodesics. The geodesic equation that we use to determine
    their evolution is
    \begin{align}\label{geodesic_eqn_1}
        \frac{dP^\mu}{d\lambda}+\Gamma^\mu_{\nu\sigma}P^\nu P^\sigma=0
    \end{align}
    where $P^a$ is the photon's $4$-momentum, and $\lambda$ is an affine parameter
    for the path.
    The Christoffel symbols, $\Gamma^\mu_{\nu\sigma}$ are functions of the
    metric, and are used to build the $G_{\mu\nu}$ that appeared in the Einstein equations~\eqref{einstein_equations}
    \begin{align}
        \Gamma^\mu_{\nu\sigma} &= \frac{1}{2}g^{\mu\gamma}\left(
        \frac{\partial g_{\gamma\nu}}{\partial x^{\sigma}}
        +\frac{\partial g_{\gamma\sigma}}{\partial x^{\nu}}
        -\frac{\partial g_{\nu\sigma}}{\partial x^{\gamma}}
        \right)\label{christoffel_symbols}\\
        R_{\mu \vu} &= \partial_\mu \Gamma^\mu_{\vu \sigma}-\partial_j\Gamma^\mu_{\sigma\mu}+
        \Gamma^\mu_{\mu \tau}\Gamma^\tau_{\vu \sigma}-\Gamma^\mu_{\vu \tau}\Gamma^\tau_{\mu \sigma}\label{ricci_tensor}\\
        G_{\mu\nu} &= R_{\mu\nu} - \frac{1}{2}g_{\mu\nu}R\label{einstein_tensor}
    \end{align}
    where $R_{\mu \nu}$ is known as the Ricci curvature tensor, and $R$ is its trace.
    Using the chain rule,~\eqref{geodesic_eqn_1} can be rewritten in terms of $t$
    \begin{align}
        P^{0}\frac{\partial P^\mu}{\partial t}+\Gamma^\mu_{\nu\sigma}P^\nu P^\sigma=0.
    \end{align}
    Writing the components of the photon's null vector 4-momentum $P^\mu$
    as $(p, \mathbf{p})$, and evaluating~\eqref{christoffel_symbols}
    for the metric~\eqref{flrw_metric} we get that
    \begin{align}\label{eq:redshifting}
        p\propto \frac{1}{a}.
    \end{align}
    The interpretation of this result is that as the universe expands,
    the photon's measured energy decreases, and its measured wavelength increases.
    %The geodesic equation~\eqref{geodesic_eqn_1} encodes many other physical effects when applied
    %to the evolution of perturbations around the homogeneous background
    %(i.e.\, allowing $g_{\mu\nu}$ to deviate from homogeneity)
    %allowing us to understand how the statistics of the perturbations at the
    %start of the $\lcdm$ evolution evolved into the perturbations we see today.


    Using~\eqref{continuity_equation} and the equation of state ($w$, summarised in table~\ref{lcdm_dep_table})
    for matter, radiation and dark energy (described as a cosmological constant $\Lambda$)
    we can solve for the dependence of $\rho_X$ on $a$.
    For matter, we obtain the simple result that $\rho_m\propto a^{-3}$,
    i.e.\ that it dilutes with the expansion of the universe.
    For radiation we find $\rho_r\propto a^{-4}$,
    i.e.\ that it dilutes with the expansion of the universe,
    and also loses energy as it redshifts, as we
    saw in~\eqref{eq:redshifting}.
    For $\Lambda$, we find $\rho_\Lambda\propto a^0$, by design.
    The Friedmann equation~\eqref{friedmann_1} then gives the time dependence of
    $a(t)$, again summarised in table~\ref{lcdm_dep_table}.


    Using this knowledge,
    and choosing our scaling such that $a_0=1$,
    we can rewrite the Friedmann equation~\eqref{friedmann_1} as
    \begin{align}\label{friedmann_omega}
        H^2 = H_0^2\left(\frac{\Omega_{r,0}}{a^4}+\frac{\Omega_{m,0}}{a^3}+\Omega_{\Lambda}\right)
    \end{align}
    where we define the fractional density
    \begin{align}
        \Omega_{X} = \frac{\rho_X}{\rho_{crit,0}}
    \end{align}
    for $X$ being radiation, matter and dark energy.
    A subscript of $0$ will denote a quantity evaluated today.
    The critical density $\rho_{crit,0}$ is the density for which the universe is flat,
    so for our universe $\sum\Omega_{i,0}=1$.
    The measured values for these quantities, as presented in~\cite{Planck_parameters_2018},
    are
    \begin{align}\label{measured_params}
        \Omega_{m,0} &= 0.3111 \pm 0.0056,\\
        \Omega_{\Lambda} &=  0.6889 \pm 0.0056.
    \end{align}
    The temperature of the blackbody $\cmb$ radiation has also been
    found to be $T_{\gamma,0}=2.725\pm0.001K$~\cite{Fixsen_2009}.
    %from which
    %we can calculate $\Omega_{\gamma,0}=9.4\times10^{-5}$.
    %Since
    %$\rho = \frac{\pi^2}{15}T^4$
    %for photons, and $T_{\gamma}=2.725\pm0.001K$
    %we have that $\rho_{\gamma}=2.02\times 10^{-15} eV^4$
    %and $\rho_{crit}=9\times 10^{-27} kg~m^{-3}$.
    %Therefore, $\Omega_{\gamma,0}h^2=2.47\times 10^{-5}$.


    Given these numbers, and~\eqref{friedmann_omega}, we can make some statements
    about the past, present and future of the universe, illustrated in figure~\ref{fig:lcdm_components}.
    Firstly, the present---we see that the cosmological constant,
    $\Lambda$, is dominant now. We also see that the energy density of radiation
    is very small, $\Omega_{\gamma,0}\ll\Omega_{m,0}$.
    Looking to the past, we see that the transition from matter domination
    to $\Lambda$ domination happened relatively recently in the history of the universe.
    We can see that this domination will continue as the matter and radiation content of the
    universe dilute and redshift away.
    Running the clock backwards, that is towards $a=0$, we see that the matter and radiation had
    higher energy densities in the early universe. Running the clock back far enough, we see
    that despite $\Omega_{\gamma,0}\ll\Omega_{m,0}$ today, at one stage it was the
    case that $\Omega_{\gamma}\gg\Omega_{m}$, and the universe was dominated by radiation.
    This epoch is the limit on how far back in time we can see---the density of free electrons
    was so high that the mean free path of photons was very short. Once the temperature
    dropped enough that neutral atoms formed, the mean free path of photons became long
    enough that they could travel cosmological distances, and eventually fall into our telescopes.
    %In that early stage of the universe, the temperature was high enough that the photons
    %were tightly coupled to the baryons. The photon pressure supported pressure waves in that
    %nearly-homogeneous fluid, and the coupling meant that the baryons were dragged along with those
    %waves. Once the temperature and density dropped sufficiently that this was no longer the case,
    %the baryons were dropped, and instead began to fall back into the gravitational wells that the dark matter
    %had meanwhile been forming. The imprints of those pressure waves is thus seen in the positions
    %of galaxies today (in the statistics of their separations) and in the acoustic oscillations of the $\cmb$.


    The $\lcdm$ model of the history of the universe takes this point as its initial condition.
    In the distant past, the components of the universe (all the different types of matter and radiation)
    were in thermal equilibrium with each other---that is, they were interacting with each other,
    and the interaction was sufficiently efficient that their temperatures matched
    and the net flow of thermal energy between them was zero.


    The evidence for this story is strong. It includes
    predictions for the relative abundances of the lightest
    elements (which were forged in the first few minutes of the universe)
    and ``baryon acoustic oscillations'' in the $\cmb$ and large scale structure ($\lss$).
    These can be directly linked to the end of the coupling
    between the baryons and the acoustic waves in the radiation,
    before the release of the $\cmb$.
    %so densely packed with radiation that it could
    %support acoustic pressure waves, and a universe that had dropped below that density.
    There are however unanswered questions within the $\lcdm$ model.
    One is the Hubble tension, a question about the
    precise rate of expansion of the universe today---see for example~\cite{tensions_2019, Freedman_2021}.
    Despite the remaining questions, it is incredible that only a century ago the cutting edge debate was
    whether the universe had a beginning or not---compared to
    now, where the debate rages over the second and
    third significant figures of its age.


    Before focusing our whole attention on the past in the rest of this thesis, let us briefly
    look toward the future of the universe.
    What will we see as the universe evolves?
    Eventually $a$ will be
    sufficiently large that $\Omega_{\Lambda}$
    is the dominant contribution to the energy budget of the universe.
    Then
    \begin{align}
        H^2 &= H_0^2\Omega_{\Lambda}\\
        \implies \dot{a} &= \pm H_0\sqrt{\Omega_{\Lambda}}a
    \end{align}
    from which the initial conditions pick out the exponentially expanding solution,\\
    ${a(t)=a_0e^{H_0\sqrt{\Omega_{\Lambda}}\left(t-t_0\right)}}$. Using the Friedmann equation
    for sufficiently far in the future we can rewrite this as
    \begin{align}
        a(t)=a_0e^{H\left(t-t_0\right)}
    \end{align}
    where $H$ is a constant, having frozen in to a factor $\sqrt{\Omega_{\Lambda}}$ smaller than
    its value today.
    One question we can ask is how far light will travel in the lifetime of the universe.
    Since $ds^2=0$ for a photon, if we define the conformal time $\tau$ such that
    \begin{align}\label{conformal_time_defn}
        \frac{d\tau}{dt} = \frac{1}{a},
    \end{align}
    we find that for the photon travelling in the $x$ direction, $dx=d\tau$.
    This means that on a plot of $x$ vs. $\tau$, the path of a photon
    will be a line of a slope of unity, even in an expanding universe.
    Integrating, we then find that
    \begin{align}
        \tau(t)=x(t)=(Ha_0)^{-1}\left(1-e^{-H(t-t_0)}\right)+\tau_0
    \end{align}
    where as usual a $0$ subscript denotes a quantity evaluated today.
    We see that the comoving distance the photon will travel is finite, even though the physical
    distance (and the time taken, $\int dt$) diverges.
    Integrating~\eqref{friedmann_omega} in the same way that generated figure~\ref{fig:lcdm_tau}
    we find that the points that we see in today's $\cmb$ are a comoving distance of $14.2\Gpc$ away.
    Since $a_0=1$, the present physical distance to these points is equal to the comoving distance,
    but this will increase exponentially in the future.
    We also find the asymptotic value of $\tau$ to be $19.2\Gpc$, indicating that we will never see the
    light that was emitted from the $\cmb$ at comoving distances greater than this.

\begin{figure}[!pth]
\centering     %%% not \center
    \includegraphics[width=.75\columnwidth]{plots/lcdm_components.png}
\caption{
    The evolution of the components of the universe up to the present, and slightly beyond.
    The vertical grey lines mark matter-radiation equality, matter-dark energy equality,
    and the present day, respectively.
    %We measure these quantities at the present, and
    %extrapolate into the past and future using the Friedmann equation~\eqref{friedmann_omega}.
    These quantities are evolved using~\eqref{friedmann_omega} and~\eqref{continuity_equation}.
    In the past, densities of matter and radiation were
    far higher, and the radiation energy density dominated over the matter.
    During this high density epoch, the expansion of the universe ($H(t)$) was far stronger.
    In the future, as dark energy comes to dominate, $H(t)$ will become constant.
}\label{fig:lcdm_components}
\end{figure}
\begin{figure}[!pth]
\centering     %%% not \center
    \includegraphics[width=.75\columnwidth]{plots/lcdm_a.png}
\caption{
    The evolution of the scale factor. For most of the $\lcdm$ history
    it evolves as some power of $t$ (see table~\ref{lcdm_dep_table}),
    however as $\Lambda$ comes to dominate
    it will begin to grow exponentially.
}\label{fig:lcdm_a}
\end{figure}
\begin{figure}[!pth]
\centering     %%% not \center
    \includegraphics[width=.75\columnwidth]{plots/lcdm_adot.png}
\caption{
    During the radiation and matter dominated eras, the evolution of
    $a(t)$ as been slowing---this is decelerating expansion.
    However, as $\Lambda$ comes to dominate,
    $\dot{a}$ will begin to increase, and the universe will enter
    an epoch of accelerated expansion.
}\label{fig:lcdm_adot}
\end{figure}
\begin{figure}[!pth]
\centering     %%% not \center
    \includegraphics[width=.75\columnwidth]{plots/lcdm_components_linear.png}
\caption{
    The evolution of the components of the universe, zoomed in to more clearly
    show the matter-dark energy transition. The first vertical grey line is
    matter-dark energy equality, the second is the present day.
}\label{fig:lcdm_components_linear}
\end{figure}
\begin{figure}[!pth]
\centering     %%% not \center
    \includegraphics[width=.75\columnwidth]{plots/lcdm_tau.png}
\caption{
    The evolution of the conformal time $\tau$ during $\lcdm$.
    We see that
    due to the eventual $\Lambda$ dominance,
    $\tau$ will asymptote to a constant,
    denoted here by the horizontal grey line at $\tau=\tau_0+(H_0a_0)^{-1}$.
    %Thus, without inflation, there is a maximum scale on which we would expect the
    %$\cmb$ to be correlated, and 
    Thus, there is a maximum comoving distance that we can ever
    expect to receive $\cmb$ photons from.
}\label{fig:lcdm_tau}
\end{figure}
%\begin{figure}[!pth]
%\centering     %%% not \center
%    \includegraphics[width=.75\columnwidth]{plots/lcdm_z.png}
%\caption{
%    The correspondence between redshift $z$ and $t$.
%}\label{fig:lcdm_z}
%\end{figure}


\section{Initial conditions for $\lcdm$}
    \subsection{Motivations for inflation}
    %The $\lcdm$ model does not posit a ``Big Bang singularity''---instead, as we have discussed, it
    %posits an early epoch in which the components that we have discussed are in thermal equilibrium,
    %at an incredibly hot temperature.
    %However, if one assumes that those components are all there are, and runs the equations backwards,
    %one reaches $a=0$ in finite time.
    The $\lcdm$ model posits an early epoch in which the components that
    we have discussed are in thermal equilibrium,
    at an incredibly hot temperature.
    These components were distributed incredibly uniformly, but with some small perturbations.
    This simple scenario then evolves under the expansion of the universe
    and the gravitational collapse of perturbations (and other interactions) and forms the universe we know.
    Is this story of $\lcdm$ enough to explain all the features of the
    universe that we observe?
    It turns out that the answer is only yes if the initial conditions for the $\lcdm$
    model had certain special characteristics.
    These special characteristics then become clues to the epoch of the universe
    that preceded $\lcdm$.


    Two of the clues that we have are known as the horizon problem, and the flatness problem.
    Roughly speaking, the first is the problem that the universe is more homogeneous on
    large scales than we would expect, given the history of $\lcdm$. Why is the temperature on one
    part of the sky so close to the temperature on the other side of the sky?
    The second is the problem that the curvature of the universe appears to be so
    small, despite the fact that there is no reason to believe it should be set to precisely zero.
    %, and one would expect the dynamics to increase it's relative contribution to the
    %energy density of the universe.
    We will not go into the details of the flatness problem except to state that
    a natural explanation of this would be an attractive quality in a
    theory of the very early universe.


    Since $a(\tau)\propto \tau^2$ during the matter dominated era we can approximate
    the total comoving distance a photon could have traveled between the singularity at $a_i=0$
    and recombination at $a_{rec}=1/1100$. We find
    \begin{align}
        x_{rec} = \int_{0}^{\tau_{rec}} d \tau = \int_{0}^{a_{rec}} \frac{d\tau}{da}da \propto \sqrt{a_{rec}}.
    \end{align}
    We can see that this is finite.
    Including the radiation dominated period does not change this conclusion---we
    can find the conformal distance $x_{rec}=280\Mpc$.
    % Compared to the physical = 278Mpc/1100 = 310kpc.
    We can also calculate that the comoving distance that a photon could have
    freely streamed through the universe since
    last scattering is $x_0=14000\Mpc$ (this matches the physical distance
    since $a_0=1$).
    This means that we would expect the homogeneous patches at recombination to span
    at most an angle of
    \begin{align}
        \tan^{-1}\left(2\frac{x_{rec}-0}{x_{0}-x_{rec}}\right) \approx 2^{\circ}
    \end{align}
    % 2.34
    on the $\cmb$ sky today.
    % Slight disagreement with Weinberg. I think
    % due to neglecting radiation and dark energy, and/or the factor of 2.
    We can also calculate the number of disconnected patches
    we should see as
    \begin{align}
        \frac{4\pi(x_{0}-x_{rec})^2}{\pi(x_{rec}-0)^2} \approx 10^5.
    \end{align}
    This is completely at odds with observations---the $\cmb$ is in fact very close to
    homogeneous \textit{across the entire sky}, clearly a feature of our universe
    that requires an explanation.


    %Photons travelling on $45'$ lines on a $\tau-x$ plot.
    While in the basic $\lcdm$ model there has not been enough conformal time since $a=0$ for
    opposite sides of the $\cmb$ to come into thermal equilibrium, this can be remedied by
    adding an epoch that causes the horizon (the characteristic conformal length scale $(aH)^{-1}$)
    to shrink---this would give large scales enough time
    to come into thermal equilibrium, then leave them outside the horizon. The usual $\lcdm$ evolution
    would then come into play, bringing these scales back inside the horizon where we can observe them.
    One way of doing this is via an effective cosmological constant---in that case the horizon $1/(aH)$ shrinks
    exponentially. We then find that $\tau=-1/(aH)$, so in the limit $a\rightarrow 0$ we see there is
    infinite conformal time in the past, to allow the entire observable universe to come into
    causal contact.


    %The flatness problem is the statement that our universe is much flatter that one would expect.
    %In~\eqref{flrw_metric}, the possibility exists that $dS_3^2$ is not the simple flat form we
    %see observationally.
    %One can write down this curvature as a contribution on the right hand side of~\eqref{friedmann_omega},
    %taking the form $\Omega_K=\frac{-K}{a^2H_0^2}$.
    %One then observes that this relative contribution would be expected to increase with time,
    %as it drops off more slowly than either matter or radiation.
    %This means that if the universe is very close to flat now (as we measure it to be)
    %then it must have been surprisingly close to flat at the beginning of the $\lcdm$
    %evolution. A natural explanation of this would be an attractive quality in a
    %theory of the very early universe.


    We will mention three more features of the $\lcdm$ initial conditions that we would
    like an explanation for. Those three features are the adiabaticity of the initial
    perturbations, the (as measured so far) Gaussianity of those perturbations, and finally
    the very slight deviation from perfect scale invariance that observations demand the
    initial power spectrum must have had.


    Adiabaticity is the statement that for each of the components of the universe, their
    initial conditions $\delta\rho_i$ were related such that
    \begin{align}
        \frac{\delta\rho_i}{\dot{\bar{\rho}}_i} = \frac{\delta\rho_j}{\dot{\bar{\rho}}_j}.
    \end{align}
    To phrase this another way, we can choose a local reparametrisation of time $t\rightarrow t+\delta t(x)$
    with $\delta t(x)=\frac{\delta\rho_i}{\dot{\bar{\rho}}_i}$, so that
    \begin{align}
        \bar{\rho}_i(t)+\delta \rho_i(t,x) = \bar{\rho}_i(t)+\delta t(x)\dot{\bar{\rho}}_i
        = \bar{\rho}_i(t+\delta t(x))
    \end{align}
    and thus the reparametrisation can absorb the perturbations in
    \textit{all} of the components.
    An implication we can draw from this is that a single degree of freedom
    would be sufficient to lay down these initial perturbations.
    The Gaussianity of the initial conditions is the statement that their statistics are completely described by
    their two-point function. We will discuss this in more detail in a later section.
    Later in section~\ref{corr_functions} we will define scale invariance,
    the statement that correlations do not depend on the scale at which they are measured.
    It turns out that observations show that this is not precisely respected,
    with the deviation from perfect scale invariance being measured to high confidence by the $\planck$
    satellite.
    Earlier measurements were made by the WMAP satellite~\cite{Senatore_wmap_2009}.


    \subsection{Driving inflation with a scalar field}
    These features of the early universe can be explained by a period of near-exponential expansion
    preceding the usual $\lcdm$ evolution.
    One simple way one could imagine driving this expansion is through a single
    scalar field.
Consider an inflaton action of the form
\begin{align}\label{inflaton_action}
S = \int d^4x \sqrt{-g}P(X,\phi)
\end{align}
with $\phi=\phi(t)$ and $X\equiv-\frac{1}{2}g^{\mu\nu}\partial_\mu\phi\partial_\nu\phi=\frac{1}{2}\dphi^2$.
We will work with the number of e-folds, $N$, as our time variable
$x'\equiv\frac{dx}{dN}=H^{-1}\frac{dx}{dt}$.


The energy-momentum tensor is
\begin{align}\label{gr_energy}
    T_{\mu\nu} = -2\frac{\partial\mathcal{L}_m}{\partial g^{\mu\nu}}+g_{\mu\nu}\mathcal{L}_m.
\end{align}
The expression for the energy density then follows as
\begin{align}\label{rho_px}
    \rho = 2XP,_X-P,
\end{align}
so then the continuity equation~\eqref{continuity_equation} implies
\begin{align}\label{rho_deriv}
    \rho' = -6XP,_X.
\end{align}
    The sound speed for the adiabatic perturbations is defined as~\cite{Christopherson_2009}
    \begin{align}\label{sound_speed_definition}
        c_s^2 = \left. \frac{\partial P}{\partial \rho} \right|_S = \frac{\dot{P}}{\dot{\rho}}
        = \frac{P,_X}{\rho,_X}.
    \end{align}
    Since we also have~\eqref{rho_px}, we see that
    \begin{align}
        c_s^2 = \frac{P,_X}{P_X+2X P_{XX}}.
    \end{align}
    For canonical inflation models, $P(X,\phi)=X-V(\phi)$
    so $P_{XX}=0$ and $c_s=1$.
    For reasons we will discuss in section~\ref{pert_evol} we will
    define a quantity $\tau_s$
    in analogy with the usual $\tau$ defined in~\eqref{conformal_time_defn}:
    \begin{align}\label{tausdef}
        \tau_s'&=\frac{c_s}{aH}.
    \end{align}
We also define the standard ``slow-roll'' parameters:
\begin{align}\label{slowrollparams}
    \eps &= -\frac{d\ln H}{dN}	\\
    \eta &= +\frac{d\ln \varepsilon}{dN}\\
    \eps_s &= +\frac{d\ln c_s}{dN}
\end{align}
though we have not demanded that these be small.
From the continuity equation and the Friedmann equation
we find the useful relation
\begin{align}\label{epsilon_phi}
    %&&\rho' &= -3(\rho+P)\\
    %\implies&&6HH' &= -3(2X P,_{X})\\
    %\implies&&-H^2\varepsilon &= -\left(\frac{1}{2}H^2{\phi'}^2\right) P,_{X}\\
    %\implies&&\varepsilon &= \frac{1}{2}{\phi'}^2 P,_{X}.
    \varepsilon &= \frac{1}{2}{\phi'}^2 P,_{X}.
\end{align}
Using~\eqref{rho_deriv}, we see that
the equation of motion for $\phi$ is~\cite{Hu_2011}
\begin{align}\label{phieom}
    \phi''+(3c_s^2-\varepsilon)\phi'+H^{-2}\frac{\rho_\phi}{\rho_X}=0.
\end{align}
In the canonical case~\eqref{epsilon_phi} simplifies to
$\varepsilon = \frac{1}{2}{\phi'}^2$,
while in the $\dbi$ case we find
$\varepsilon = \frac{1}{2}\frac{{\phi'}^2}{c_s}$.
We will now briefly discuss the special case of canonical models,
with $P(X,\phi)=X-V(\phi)$.
    For a canonical model this scalar field would have
    \begin{align}
        \rho_\phi &= \frac{1}{2}\dot{\phi}^2+V(\phi)\\
        P_\phi &= \frac{1}{2}\dot{\phi}^2-V(\phi)\\
        \implies H^2 &= \frac{V(\phi)}{3-\varepsilon}
    \end{align}
    where the last equality used the Friedmann~\eqref{friedmann_1} equation and
    has not assumed $\varepsilon$ is small.


    For successful inflation with the potential of the scalar field acting as a
    cosmological constant, we want $w\approx-1$,
    i.e.\ that $V(\phi)\gg\frac{1}{2}\dot{\phi}^2=\varepsilon H^2$. Since the kinetic term is required to
    be small, this is referred to as \textit{slow-roll} inflation.
We can also see that requiring $\rho_\phi=-p_\phi$ using~\eqref{rho_px} demands that
$2XP_{,X}\ll P$.
    For a canonical model in slow-roll,
    the Friedmann equations then become
    \begin{align}
        H^2 &\approx \frac{8\pi G}{3}V(\phi),\\
        \frac{\ddot{a}}{a} &\approx -\frac{4\pi G}{3}\left(-2V(\phi)\right).
    \end{align}

%\subsection{Criteria for successful inflation}
    For inflation to solve the horizon problem, it must result in a shrinking comoving
    Hubble radius, disconnecting regions that had previously been in thermal contact.
    This implies that
    \begin{align}
        \frac{d}{dt}\left(aH\right)^{-1} = -\frac{\ddot{a}}{\dot{a}^2} < 0
    \end{align}
    so $\ddot{a}>0$, i.e.\ the expansion is accelerating. Another way of writing this is
    \begin{align}
        \frac{d}{dt}\left(aH\right)^{-1} &= -\frac{1}{(aH)^2}\left(\dot{a}H+a\dot{H}\right)\\
            &= -\frac{1}{a}\left(1-\varepsilon\right)
    \end{align}
    and so we need $\varepsilon<1$ to successfully inflate the universe.
    However we see that for near-exponential expansion we have already demanded
    $\varepsilon\equiv-\dot{H}/H^2\ll1$ so this must be satisfied.
    %This is necessary but not sufficient---we still need to generate
    %the correct statistics for the primordial perturbations.
    %This implies we must be close to a perfect de Sitter phase, i.e.\ $\varepsilon\ll1$.
    To match the measured deviation from scale-invariance in standard slow-roll
    inflation it turns out we would need $\varepsilon\sim O(10^{-2})$.


For a model with a canonical kinetic term,~\eqref{phieom} simplifies
to
\begin{align}
    \phi''+(3-\varepsilon)\phi'+H^{-2}V_{\phi}(\phi)=0.
\end{align}
In the slow-roll approximation we take assume $\phi''\ll\phi'$,
and so
\begin{align}
    \phi'\approx\frac{V_{\phi}(\phi)}{3H^2}\approx\frac{V_{\phi}(\phi)}{V(\phi)}.
\end{align}
From this, we see that demanding that $\varepsilon$ be small places a constraint
on the flatness of the potential in a canonical model.
Requiring inflation to last for the sufficient number
of e-folds to solve the horizon problem
also constrains $\eta\ll1$.


%This allows us to rephrase the Friedmann equation in the following useful
%way. Note that this is still exact, the slow-roll approximation has
%not been used.
% WRONG
%\begin{align}
%    &&H^2 &= \frac{1}{3}\left(\frac{\varepsilon H^2}{P,_{X}}+V(\phi)\right)\\
%    \implies &&  H^2 &= \frac{V(\phi)}{3-\frac{\varepsilon}{P,_{X}}}.
%\end{align}


\section{Statistical observables}
\subsection{From probabilities to observations}\label{corr_functions}
    The discussion until this point has focused on quantities
    with definite values. This will no longer
    be true when we discuss quantities that have a quantum origin, as
    the prediction of a fundamental quantum theory is a statistical one.
    The true prediction is of a distribution from which our observation will be drawn;
    as such, to link the sky we see to some fundamental theory,
    we must talk in terms of probabilities.


    We begin with a brief summary of some probabilistic quantities.
    We can define the expectation value of a function $f$ of a discrete random variable $x$,
    the expectation value of a function $f$ of a continuous random variable $x$,
    or a functional $F$ of a field configuration $f(x)$, respectively as
    \begin{align}
        \left<f(x)\right> &= \sum_i f(x_i) P(x_i)\label{expectation_value_discrete}\\
        \left<f(x)\right> &= \int dx~f(x) \rho(x)\label{expectation_value_cont}\\
        \left<F\left[f\right]\right> &= \int \mathcal{D}f~F\left[f\right] P\left[f\right]\label{expectation_value_field}
    \end{align}
    where the sum and integral over $x$ are over all the values that $x$ can take,
    and the functional integral $\int \mathcal{D}f$ over $f$ is over all the field configurations
    that $f(x)$ can take.


    %In our example of the $100$ coin flips, we used~\eqref{expectation_value_discrete}
    %to calculate the mean and standard deviation.
    For example, if one wanted to
    calculate the expected number of heads in $100$
    coin flips (or the variance in that number),
    one would use~\eqref{expectation_value_discrete}.
    %For a continuous variable,
    %like the average height of a population or the average temperature in a given room,
    %we would use~\eqref{expectation_value_cont}.
    In this thesis, we will be working with the expectation value of
    functions of field configurations, so will be working with quantities
    defined in terms of~\eqref{expectation_value_field}.
    In particular, the quantum mechanical nature of the inflationary
    origin of structure means that such predictions
    of inflationary theories will necessarily be statistical. That is to say, the predictions through
    which we will test these theories will be predictions of these expectation
    values of the primordial perturbations.


    The ensemble average~\eqref{expectation_value_field} of
    $f(\mathbf{x})f(\mathbf{x'})$ for some function $f$ is known as the
    two-point correlator, or the two-point function: $\left<f(\mathbf{x})f(\mathbf{x'})\right>$.
    Note the brackets $\left<\ldots\right>$ refer to the expected value over the ensemble,
    over all the possible realisations that could be drawn from the probability
    distribution, and not a spatial average.
    %predicted by the inflation model, weighted by their probability.
    Higher order correlators are the
    expectation values of products the field evaluated at more points.


    There are two assumptions we wish to immediately impose on such functions.
    Those conditions are statistical homogeneity and statistical isotropy,
    the requirement that the correlators are invariant under translations and rotations.
    This does not mean that some realisation of the ensemble is forbidden from
    breaking these symmetries, of course---fields
    are still allowed to be inhomogeneous, and thus (for example) have local extrema.
    The statement of statistical homogeneity is that,
    when averaged over the entire ensemble of possible realisations, these special points
    are equally likely to appear anywhere in space.
    All observations of the universe so far appear to respect these two symmetries of the statistics.


    These conditions on the correlators of $f(\vecx)$ place useful constraints on the correlators of the
    Fourier transform, $f(\veck)$. They demand the form
    \begin{align}\label{spectrum_definitions}
        \left<f(\mathbf{k_1})f(\mathbf{k_2})\right> &= (2\pi)^3\delta^{(3)}(\mathbf{k_1}+\mathbf{k_2})P(k_1),\\
        \left<f(\mathbf{k_1})f(\mathbf{k_2})f(\mathbf{k_3})\right> &= (2\pi)^3\delta^{(3)}(\mathbf{k_1}+\mathbf{k_2}+\mathbf{k_3})B(k_1,k_2,k_3),
    \end{align}
    where $P(k)$ is known as the power spectrum and $B(k_1,k_2,k_3)$ is the bispectrum.
    The delta functions come from statistical homogeneity, while the fact that $P(k)$
    and $B(k_1,k_2,k_3)$ depend only on the magnitudes of the vectors ($k=\left|\veck\right|$) is a result
    of demanding statistical isotropy.
    Note that $\left<f(\mathbf{k_1})f(\mathbf{k_2})f(\mathbf{k_3})\right>$
    has $9$ apparent degrees of freedom, but $B(k_1,k_2,k_3)$ has only three.
    This is due to the delta function
    demanding that the sum of the $\mathbf{k_i}$ vanishes, thus forming a triangle.
    Since the orientation of this triangle
    cannot matter, three parameters will suffice---we choose the magnitudes of the vectors.
    Similar statements can of course be made for higher point correlators.


    Our focus in this thesis will be on the predictions that models of inflation make
    for the the bispectrum $B(k_1,k_2,k_3)$.
    This is one characteristic property of 
    the initial conditions for the $\lcdm$ evolution that follows after. From
    this, one can calculate statistical properties of the $\cmb$ sky that we see.
    If the bispectrum has the form
    \begin{equation}\label{sepXYZ}
        B(k_1,k_2,k_3) = X(k_1)Y(k_2)Z(k_3),
    \end{equation}
    or can be expressed as a sum of such terms, it is called separable.
    This property allows useful simplifications to such calculations, that
    will be explored in chapter~\ref{chapter:intro_bispectra}.


    Another symmetry one could imagine is statistical scale invariance.
    Scale invariance is the statement that
    \begin{align}
        \left<f(\mathbf{x})f(\mathbf{x'})\right> = \left<f(\lambda\mathbf{x})f(\lambda\mathbf{x'})\right>.
    \end{align}
    One can show that this symmetry demands $P(k)\propto k^{-3}$.
    Motivated by this, it is usual to define the dimensionless power spectrum
    \begin{align}\label{primpowerspec_defn}
        \mathcal{P}(k) = \frac{k^3}{2\pi^2}P(k).
    \end{align}
    % By looking at <f(x)^2>
    Evidence suggests that this symmetry is \textit{not} respected by the probability
    distribution from which our universe was drawn.
    In fact, it has been estimated that $P(k)\propto k^{n_s-4}$, with $n_s=0.9649 \pm 0.0042$
    at $68\%$ CL~\cite{Planck_inflation_2015, Planck_inflation_2018}.
    Thus we can make the statement that
    the odds of a universe like ours being drawn from a scale-invariant probability
    distribution are negligibly small.


    We will close this section with a brief note on Monte Carlo simulations.
    Using these simulations, for some given probability distribution, we can determine
    what our observations might look like---even in the presence of
    complicating factors such as, in the case of observing the harmonics of the temperature fluctuations
    in the $\cmb$, the presence of our galaxy blocking a significant portion
    of the sky.
    For example, if one makes the assumption that the amplitudes of
    such harmonics were drawn from a Gaussian distribution with some given power spectrum,
    one can then generate a large ensemble of possible skies.
    Processing these by simulating the known instrument effects and
    sky masks gives an ensemble of realistic experimental data, drawn
    from a Gaussian distribution. If one then wanted to test the temperature
    fluctuations of the $\cmb$ for deviations from Gaussianity,
    for example, one could use this ensemble to estimate the rate of false positives
    of that test.
    Thus, Monte Carlo simulations provide a way to generate possible observations,
    even in complicated situations,
    when the probability distribution is assumed.


    %One such statistical property is the two-point correlation of the temperature $\phi$
    %between two points $x$ and $x'$ in space, $\left<\phi(x)\phi(x')\right>$.
    %We will want to measure these correlations. How can we do that with only access to one realisation
    %of the distribution? It is useful to think about the ergodic theorem. Since an integral over
    %space can be identified with the ensemble average, we can measure, say, the average temperature
    %across space, which should give us an estimate for the average temperature of the ensemble
    %(the quantity that will actually be predicted from theory).
    %Is this related to large cosmic variance at large scales? See pg 94 of Lyth and Liddle.

\subsection{From observations to probabilities}
    Instead of describing observations generated from known probabilities,
    we usually wish to do the opposite.
    We wish to take a series of observations of some random variable and use these observations
    to understand the probability distribution that they were drawn from.
    %Correlation functions are expectation values over ensembles of possible
    %universes, and characterise such probability distributions.
    It is reasonable to wonder how we could ever make contact with a theory
    that only makes these kind of predictions, given that we only have access to one universe.
    Are we doomed from the start? Thankfully, it turns out, that is not the case.


    Inflation is the theory we use to calculate a probability distribution
    that our universe may have been drawn from.
    %If we only have a finite amount of data from this ensemble,
    %we cannot perfectly measure some given average, as we will be limited by cosmic variance.
    While we may not have access to other universes drawn from the same distribution,
    one can ask how much information we can glean from the one we do have.
    For example, if we magically had access to the entirety of one spatially
    infinite universe then what would our measured spatial averages tell us?
    Would those averages match the ensemble averages?
    If the distribution is such that the correlations between the field evaluated at distant
    points goes to zero sufficiently fast, then we could intuitively imagine that the answer is
    yes---since distant regions are uncorrelated, they are effectively separate draws, and
    so a spatially infinite universe will give us enough information to measure the true
    ensemble average. This intuition is made precise by the ergodic theorem
    (see for example appendix D of~\cite{Weinberg_cosmo}).
    If the assumptions of the ergodic theorem are met,
    then spatial averages and ensemble averages
    match---despite the fact we only have one universe,
    if we had the whole spatial infinity of that universe
    we would still be able to measure the ensemble average.
    Of course, we do not have access to spatial infinity,
    thus our spatial averages will only ever be an approximation of the
    true ensemble averages---this is known as cosmic variance.
    To summarise,
    there are two ways to beat this cosmic variance (to measure an ensemble
    average $<f(\vecx)^2>$ at some fixed $\vecx$, say).
    We could stay in the same spatial region, but magically peek into all the other universes.
    Or, we could stay in the same universe, but magically peek into every spatial region.
    The ergodic theorem tells us that these two methods will give the same answer,
    thus allowing us to make contact between spatial measurements made in
    our one universe and the ensemble predictions of inflation.


    In practice, we have other advantages.
    Even in a limited spatial region we can have access to multiple
    samples drawn independently from one distribution, for example
    in multipole moments due to statistical isotropy.


    %Our result will be a constraint on an inflationary parameter, stated at e.g.\ $95\%$ confidence.
    %For example, if we flipped one fair coin $100$ times, we would expect an even split of heads
    %and tails. Should we be suspicious of the fairness of the coin if we instead get $60$ heads
    %and $40$ tails? Or $80$ heads and $20$ tails? An event at least as extreme at a $60$-$40$ split
    %will occur $3.5\%$ of the time. For a $80$-$20$ split however, the probability of an event that
    %extreme is around one part in $10^{10}$, effectively impossible. This means that if we observed
    %a trial where a coin came up heads in $80$ out of $100$ cases, we could suspect that out hypothesis
    %of fairness was incorrect.
    %One can calculate that, for a fair coin, $95\%$ of the time the number of heads will take a value
    %between $41$ and $59$. We call this the $95\%$ confidence interval.
    %What we wish to do is the opposite, however. Instead of calculating using known probabilities,
    %we wish to take a series of observations of some random variable and use these observations
    %to estimate the probability distribution that they were drawn from. Given an infinite number
    %of observations this task has a straightforward solution, simply plotting the normalised
    %histogram of the results. However, in the context of cosmology we will only have a finite amount
    %of information available to us, and thus a limit to how well we can ever measure the relevant
    %probability distributions. This limitation is known as cosmic variance.



    %We quantify this expected scatter around the mean using a quantity known as the standard deviation.
    %The standard deviation of a random variable $X$ is the square root of the expected value of the
    %squared deviation from the mean $\mu$,
    %\begin{align}
    %    \sigma = \sqrt{E\left[{(X-\mu)}^2\right]}.
    %\end{align}
    %In our example above of flipping one coin $100$ times, the total number of heads has $\mu=50$
    %and $\sigma=5$.

    
    %This is related to the concept
    %of ergodicity. This is the statement that $\left<\cdot\right>$ as an expectation over ensembles
    %at a fixed point is the same as the expectation over points for a fixed ensemble.
    %This assumes homogeneity, stationarity, and that distant points are uncorrelated.
    %In terms of our coin example, this is related to the statement that flipping one coin
    %$100$ times should probe the probability distribution in the same way as flipping
    %$100$ identical coins once each.


    %\begin{align}
    %    \left<\hat{v}_{\bk}\hat{v}_{\bk'}\right>
    %                     &= \left<0|\hat{v}_{\bk}\hat{v}_{\bk'}|0\right>\\
    %                     &= |v_k|^2\left<0\left|\left[\hat{a}_\bk,\hat{a}_{-\bk}^{\dagger}\right]\right|0\right>\\
    %                     &= |v_k|^2\delta(\bk+\bk')\\
    %                     &= P_{v}(k)\delta(\bk+\bk')
    %\end{align}


\section{Thesis outline}
Given the context we have outlined,
    we will now give a description of the goals, methods
    and results presented in this thesis.
    The main goal discussed here is to develop an efficient numerical pipeline for
    connecting inflation models directly to observations through the $\cmb$ bispectrum.
    The concrete results of such a goal are constraints on the parameters of inflation models,
    not constraints on phenomenological templates or summary $\fnl$ parameters.
    This allows the use of the full bispectrum shape information from an inflation scenario,
    not point samples or a limit. The novelty of our results comes from the separable
    and numerical methods
    granting access to more accurate, and in some cases new, bispectrum shapes.


    To achieve these goals we have developed methods to preserve the
    separability that is built in to the tree-level in-in formalism---these
    terms will be defined in chapter~\ref{chapter:intro_bispectra}.
    This will link scenarios of inflation to the $\cmb$,
    through an estimator that will be presented in~\cite{Sohn_2021}.
    The calculations involved in that estimator are expensive,
    but need only be done once per primordial basis.
    This motivates the desire for a basis that converges quickly for a broad range of inflation models,
    and as such we have explored this topic in detail.


The first line of research presented in this thesis
is the determination of the feasibility of the overall method.
The initial result here was identifying the contributions of the non-physical configurations
as a novel problem to this formalism,
and identifying basis choice as the key method of overcoming this difficulty.
In this work we describe multiple basis sets and make quantitative comparisons
of their convergence on realistic and interesting models, including ones with features.
These basis sets can overcome the difficulty of the dominant non-physical $k$-configurations,
converging far more efficiently than the basic basis sets, for physically interesting models.


The second line of research presented is the development of
basis-independent methods that allow the fast and accurate calculation
of higher order coefficients of the basis expansion of the tree-level in-in formalism.
To this end we set up the separable formalism and describe the methods used to overcome the
difficulties encountered.
These difficulties include accurately including the highly oscillatory early-time contributions,
and difficult numerical cancellations between terms in the interaction Hamiltonian
(and corrections from field redefinitions).
A careful and comprehensive validation of our methods is presented,
showing that the convergence problems can be overcome, the oscillatory contributions captured efficiently.
At the primordial level this is done by validating our results using three distinct tests.
Validation is done on established templates with non-trivial features;
the squeezed-limit consistency condition is verified; our
full bispectrum results are compared to previous codes through point-tests.
%This is the first development of a formalism
%for calculating the separable expansion to sufficiently
%high orders as to describe inflationary features.


As a side-effect of this desire for a separable primordial bispectrum,
the primordial calculation of the inflationary bispectrum presented in this work is much more efficient
than previous methods,
in the sense that it converges far faster in modes than previous methods would in point samples.
These methods are implemented in the $\primodal$ code,
a Python code which can quickly and precisely produce a full feature bispectrum,
despite minimal optimisation.


    The final highlighted result, obtained in collaboration, is the connection
    of an inflationary scenario to the $\cmb$ through the $\cmbbest$ code\footnote{
        Developed by Wuhyun Sohn.}.
    This allowed in particular a constraint to be placed on the sound speed of $\dbi$ inflation, $c_s$.
    At the level of the CMB, validation of the method as a whole
    (when applied to the $\planck$ data)
    is achieved by comparison with the
    $\planck$ constraint on the $\dbi$ sound speed.
    This constraint is translated into a constraint on
    a fundamental parameter in the context of the inflationary parameter scan.


\bigskip


\textbf{Chapter~\ref{chapter:intro_bispectra}} of this thesis
is an introduction to the bispectrum as an observable.
The various parts of the pipeline that connects inflation scenarios to observations
through the bispectrum are reviewed.
The usual paradigm of bispectrum estimation in the CMB is outlined,
along with the motivation for separable bispectra. The in-in formalism
for calculating the tree level bispectrum for a given model of inflation
is presented in detail, and the point at which the separable formalism diverges is
highlighted.
Reviews of $P(X,\phi)$ models of inflation, and
%some of the usual approximate bispectrum templates
%that we aim to bypass.
%We will draw our validation scenarios from these models.
discussions of previous numerical codes for
calculating the primordial bispectrum $k$-configuration by $k$-configuration
are presented.
%which contrasts our separable basis expansion.
Also reviewed is the previous work in achieving separability through modal expansions
in~\cite{Funakoshi}.
%and we discuss methods of testing
%numerical bispectrum results, defining our relative difference measurement.



\textbf{Chapter~\ref{chapter:decomp}} describes the first line of research mentioned above---the
discussion covers the need to
take the non-physical configurations into account and how the basis sets
presented in this work are built.
Also presented is a quantitative comparisons of the convergence of each basis set to
relevant examples of bispectrum shapes.


%By recasting the usual in-in calculation into an explicitly separable form,
%in terms of an expansion in an arbitrary basis.
Since the paradigm presented here is only viable if one can find a basis
that can efficiently represent a wide variety of bispectra,
the majority of chapter~\ref{chapter:decomp} is devoted to presenting this exploration.
This topic is distinct and separate from our numerical methods, but nonetheless vital.
%Attention is given to the effects of the
%non-physical $k$-configurations on the convergence of our expansion on
%the tetrapyd, and multiple efficient basis sets are presented.



\textbf{Chapter~\ref{chapter:methods}} details the second line of research mentioned above---a
discussion of the details of recasting the in-in formalism in an explicitly separable form is presented,
making explicit each step of the calculation.
Methods for efficiently dealing with the early time contributions to the integrals are discussed,
as well as other numerical issues that require care and attention.
Also presented are validation tests on a very broad range of types of non-Gaussianity.


%We present the precise methods involved in the
%separable formalism and its numerical implementation,
%and detail our methods for carefully calculating the coefficients to high order.
%In particular in section~\ref{sec:validation} we validate our methods and implementation
%on inflation scenarios with varied features from the literature.


\textbf{Chapter~\ref{chapter:constraints}} presents a constraint
on the sound speed of the $\dbi$ model, which validates our pipeline against a constraint
from the $\planck$ analysis.
This chapter will describe the parametrisation of the scan,
and pay careful attention to the convergence of the result
with respect to the basis size.


\textbf{Chapter~\ref{chapter:conclusion}} discusses possible avenues of future work,
and presents our conclusions.

