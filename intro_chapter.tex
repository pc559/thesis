%\input{commands}
\chapter{Introduction to cosmology}\label{chapter:intro_general}
\section{General introduction}\label{sec:general_intro}
%\subsection{The bispectrum}
    \subsection{Fundamentals}
\textcolor{red}{Write proper English sentences, sort out ordering.
How can we know things about the history of the universe? Remove some of the plots.}
    General relativity tells us how the expansion of the universe depends
    on its matter and energy content.
    In the context of a spatially homogeneous and isotropic universe
    (known as an FLRW universe)
    the Einstein equations become the Friedmann equations.


    We begin with a homogeneous and isotropic metric with $c=1$
    \begin{align}
        ds^2 = a(t)^2 ds_3^2 - dt^2
    \end{align}
    where $a(t)$ will have the interpretation as the scale factor
    which describes the evolution of the universe.
    The components of the matter and energy content of the universe enter the equations through
    their stress-energy tensors, which we approximate as that of
    a perfect fluid
    \begin{align}
        T_{ab} = (\rho+P)U_aU_b+Pg_{ab}.
    \end{align}
    The quantity $w=\frac{P}{\rho}$ is constant,
    and takes the values $0$, $1/3$ and $-1$ for matter, radiation
    and a cosmological constant respectively.
    \textcolor{red}{Cosmological constant vs. dark energy!}
    The Einstein equation
    \begin{align}
        G_{\mu\nu} = 8\pi G T_{\mu\nu}
    \end{align}
    then gives us the Friedmann equations
    for a flat universe with $\Lambda=0$
    \begin{align}
        H^2 &= \frac{8\pi G \rho}{3},\\
        \frac{\ddot{a}}{a} &= -\frac{4\pi G}{3}\left(\rho+3P\right),
    \end{align}
    where $H=\frac{\dot{a}}{a}$, and $\rho$ and $P$ are respectively the sum of the
    energy densities and of the pressure densities of the
    components of the universe. We also have the continuity equation
    \begin{align}
        \dot{\rho} + 3\frac{\dot{a}}{a}\left(\rho+P\right) &= 0.
    \end{align}

    Given the equations of state and the densities of the different
    components of the universe, one can then calculate the time dependence of the
    scale factor $a(t)$. From this, one can understand how perturbations evolve, and understand
    how photons are redshifted as they freestream through the universe.


    Photons travel on geodesics. The geodesic equation that we use to determine
    their evolution is
    \begin{align}
        \frac{dP^a}{d\lambda}+\Gamma^a_{bc}P^bP^c=0.
    \end{align}
    Using the chain rule, this can be rewritten as
    \begin{align}
        P^{0}\frac{\partial P^a}{\partial t}+\Gamma^a_{bc}P^bP^c=0.
    \end{align}
    \textcolor{red}{REPHRASE ALL THIS}
    Writing the components of the photon's 4-momentum $P^a$
    as $(p, \mathbf{p})$ we get that
    \begin{align}
        p\propto \frac{1}{a}.
    \end{align}
    The interpretation of this result is that as the universe expands,
    the photon's energy decreases, and its wavelength increases.
    The geodesic equation encodes many other physical effects when applied
    to the evolution of perturbations around the homogeneous background,
    allowing us to understand how the statistics of the perturbations at the
    start of the $\lcdm$ evolution evolved into the perturbations we see today.


    Since the photon gas has a black body spectrum \textcolor{red}{TALK ABOUT THIS}
    and is in thermal equilibrium we can assign a temperature to it.
    This temperature evolves as $T\propto a^{-1}$ away from decoupling temperatures.


    We can rewrite the Friedmann equation as
    \begin{align}\label{friedmann_omega}
        H^2 = H_0^2\left(\frac{\Omega_{r,0}}{a^4}+\frac{\Omega_{m,0}}{a^3}+\Omega_{\Lambda}\right)
    \end{align}
    where we define the fractional density
    \begin{align}
        \Omega_{X} = \frac{\rho_X}{\rho_{crit,0}}
    \end{align}
    for $X$ being radiation, matter and dark energy, using $\rho_{crit,0}$, the critical density for which the universe is flat.


    From this equation, and the values measured values from~\cite{Planck_parameters_2018}
    \begin{align}\label{measured_params}
        \Omega_{m,0} &= 0.3111 \pm 0.0056,\\
        \Omega_{\Lambda} &=  0.6889 \pm 0.0056.
    \end{align}
    Since
    $\rho = \frac{\pi^2}{15}T^4$
    for photons, and $T_{\gamma}=2.725\pm0.001K$ \textcolor{red}{CHECK THIS},
    we have that $\rho_{\gamma}=2.02\times 10^{-15} eV^4$
    and $\rho_{crit}=9\times 10^{-27} kg~m^{-3}$.
    Therefore, $\Omega_{\gamma,0}h^2=2.47\times 10^{-5}$.
    From these numbers, and~\eqref{friedmann_omega}, we can make some statements
    about the past and future of the universe, illustrated in figure~\ref{fig:lcdm_components}.
    We can see that while $\Lambda$ is dominant now, the transition from matter domination
    to $\Lambda$ domination happened relatively recently in the history of the universe.
    We can see that this domination will continue as the matter and radiation content of the
    universe dilute and redshift away.
    Running the clock backwards, that is towards $a=0$, we see that the matter and radiation had
    higher energy densities in the early universe. Running the clock back far enough, we see
    that despite $\Omega_{\gamma,0}\ll\Omega_{m,0}$, at one stage it was the
    case that $\Omega_{\gamma}\gg\Omega_{m}$, and the universe was dominated by radiation.
    In that early stage of the universe, the temperature was high enough that the photons
    were tightly coupled to the baryons. The photon pressure supported pressure waves in that
    nearly-homogeneous fluid, and the coupling meant that the baryons were dragged along with those
    waves. Once the temperature and density dropped sufficiently that this was no longer the case,
    the baryons were dropped, and instead began to fall back into the gravitational wells that the dark matter
    had meanwhile been forming. The imprints of those pressure waves is thus seen in the positions
    of galaxies today (in the statistics of their separations) and in the acoustic oscillations of the $\cmb$.


    Let us take the future of the universe as an example, where $a$ is sufficiently large that $\Omega_{\Lambda}$
    is the dominant contribution to the expansion.
    Then
    \begin{align}
        H^2 &= H_0^2\Omega_{\Lambda}\\
        \implies \dot{a} &= \pm H_0\sqrt{\Omega_{\Lambda}}a
    \end{align}
    from which the initial conditions pick out the exponentially expanding solution,\\
    ${a(t)=a_0e^{H_0\sqrt{\Omega_{\Lambda}}\left(t-t_0\right)}}$. Using the Friedmann equation
    for sufficiently far in the future we can rewrite this as
    \begin{align}
        a(t)=a_0e^{H\left(t-t_0\right)}.
    \end{align}
    Then, using $ad\tau=dt$, we find that ${\tau(t)=(Ha_0)^{-1}\left(1-e^{-H(t-t_0)}\right)+\tau_0}$
    where as usual a $0$ subscript denotes a quantity evaluated today.
    This is the behaviour we would also like in the very early universe, as we will see.

    \begin{table}[h!]
    \begin{center}
        \begin{tabular}{ c c c }
            Epoch & $a(t)$ & $a(\tau)$ \\ 
            \toprule
            Radiation & $t^{\frac{1}{2}}$ & $\tau$ \\
            Matter & $t^{\frac{2}{3}}$ & $\tau^2$ \\
            $\Lambda$ & $e^{Ht}$ & $-\frac{1}{\tau}$
        \end{tabular}\caption{
            How the scale factor, $a(t)$, evolves in the
            different epochs of the universe.
        }\label{lcdm_dep_table}
    \end{center}
    \end{table}


    Set-up the physical effects of transfer functions.


\begin{figure}[!pth]
\centering     %%% not \center
    \includegraphics[width=.75\columnwidth]{plots/lcdm_components.png}
\caption{
    The evolution of the components of the universe up to the present, and slightly beyond.
    The vertical grey lines mark matter-radiation equality, matter-dark energy equality,
    and the present day, respectively. We measure these quantities at the present, and
    extrapolate into the past and future using the Friedmann equation~\eqref{friedmann_omega}.
    In the past, densities of matter and radiation were
    far higher, and the radiation energy density dominated over the matter.
    During this high density epoch, the expansion of the universe ($H(t)$) was far stronger.
    In the future, as dark energy comes to dominate, $H(t)$ will become constant.
}\label{fig:lcdm_components}
\end{figure}
\begin{figure}[!pth]
\centering     %%% not \center
    \includegraphics[width=.75\columnwidth]{plots/lcdm_a.png}
\caption{
    The evolution of the scale factor. For most of the $\lcdm$ history
    it evolves as some power of $t$ (see table~\ref{lcdm_dep_table}),
    however as $\Lambda$ comes to dominate
    it will begin to grow exponentially.
}\label{fig:lcdm_a}
\end{figure}
\begin{figure}[!pth]
\centering     %%% not \center
    \includegraphics[width=.75\columnwidth]{plots/lcdm_adot.png}
\caption{
    During the radiation and matter dominated eras, the evolution of
    $a(t)$ as been slowing---this is decelerating expansion.
    However, as $\Lambda$ comes to dominate,
    the $\dot{a}$ will begin to increase, and the universe will enter
    and epoch of accelerated expansion.
}\label{fig:lcdm_adot}
\end{figure}
\begin{figure}[!pth]
\centering     %%% not \center
    \includegraphics[width=.75\columnwidth]{plots/lcdm_components_linear.png}
\caption{
    The evolution of the components of the universe, zoomed in to more clearly
    show the matter-dark energy transition. The first vertical grey line is
    matter-dark energy equality, the second is the present day.
}\label{fig:lcdm_components_linear}
\end{figure}
\begin{figure}[!pth]
\centering     %%% not \center
    \includegraphics[width=.75\columnwidth]{plots/lcdm_tau.png}
\caption{
    The evolution of the conformal time $\tau$ since the $\lcdm$ singularity,
    if there is no inflation. Eventually $\tau$ will asymptote to a constant,
    denoted here by the horizontal grey line at $\tau=\tau_0+(H_0a_0)^{-1}$.
    \textcolor{red}{Physical interpretation at early and late times!!}
}\label{fig:lcdm_tau}
\end{figure}
\begin{figure}[!pth]
\centering     %%% not \center
    \includegraphics[width=.75\columnwidth]{plots/lcdm_z.png}
\caption{
    The correspondence between redshift $z$ and $t$.
    \textcolor{red}{Talk about the observational effects of having an
    evolving $\dot{a}$.}
}\label{fig:lcdm_z}
\end{figure}

    The history of the universe as a story of falling temperature,
    probing higher energy scales. Rough contrast of looking at the
    early universe with particle colliders, cosmic variance etc.
    \textcolor{red}{Write this, make consistent with elsewhere!}
    \subsection{$\lcdm$}
    \textcolor{red}{Leave this until basically the end, it's least important.
        Add references! Don't explain in detail, but do reference.}
    The $\lcdm$ model of the history of the universe assumes that at some point in the
    past, the components of the universe (all the different types of matter and radiation)
    were in thermal equilibrium with each other---that is, they were interacting with each other,
    and the interaction was sufficiently efficient that their temperatures matched
    and the net flow of thermal energy between them was zero.


    Recombination, what the $\cmb$ is.
    Plot of $a(t)$, $\tau(t)$ over all epochs.
    Path of a photon in $t-x$? Plot of $E(t)$?


    Radiation is one of the major known components of the present universe, in the form of the
    photons in the cosmic microwave background, the $\cmb$. Another is matter, by which we mean \textit{dark} matter,
    which is invisible to our telescopes (which depend on electromagnetic radiation) but can be mapped by the
    effect of its gravity[ref]. Another component is what cosmologists refer to as baryonic matter, which is
    the protons, electrons, neutrons and all the other particles which make up the visible galaxies, stars
    and humans.


    What is the evidence for $\lcdm$? Two main pieces of evidence are the relative abundances of
    primordial elements, and the baryon acoustic oscillations (BAOs).


    While it is thought that the heavier elements are made in incredibly energetic supernovae and
    neutron star collisions (such as lead[ref], or gold[ref]) the lightest elements are instead thought to have been forged in the
    very first minutes of the universes history. The $\lcdm$ model makes a detailed prediction as to the
    relative abundances of these elements, hydrogen, helium and lithium. It works well, but there is
    the ``lithium problem''.


    Baryon acoustic oscillations are the remnants of an epochal transition in the early universe.
    When the baryons decoupled from the radiation (due to the falling temperature) they were dropped,
    no longer supported by the photon pressure. They began to collapse back into the gravitational
    wells the dark matter had been forming. The imprint of this can be seen in the statistics
    of the positions of galaxies in the sky \textcolor{red}{Did I use the zooming out thing in
    BlueSci? Can I use it here??} [ref two-point], we will discuss two-point statistics in
    section \textcolor{red}{ref}.


    Future work on $\lcdm$ includes the solution of the Hubble tension---how old is the universe really?
    All in all, it is incredible only a hundred years ago people were debating whether the universe had a beginning or not
    (and the ``great debate''---were there other galaxies?) to now, where the debate rages over the second and third significant
    figures.
\newpage
\section{Initial conditions for $\lcdm$}
    \subsection{Motivations for inflation}
\textcolor{red}{This is important, but shouldn't be long!}
    The $\lcdm$ model does not posit a ``Big Bang singularity'', though this is often
    how it is described in popular science communication.
    \textcolor{red}{Is it a consequence?}
    Instead, as we have discussed, it
    posits an early epoch in which the components that we have discussed are in thermal equilibrium,
    at an incredibly hot temperature of \textcolor{red}{number}. These components are distributed incredibly uniformly,
    with inhomogeneities of less than \textcolor{red}{number}. This simple scenario then evolves
    under gravitational collapse (and other interactions) and forms the universe we know.


    Is this story of $\lcdm$ enough to explain all the features of the
    universe that we observe? The answer is mostly less, given the correct initial conditions,
    the correct starting point. But then that begs the question of how those initial conditions
    were chosen out of the other possibilities one could imagine.
    Two clues that we have are known as the horizon problem, and the flatness problem.
    Roughly speaking, these problems are the statements that the universe is more homogeneous on
    large scales than we would expect, given the history of $\lcdm$. Why is the temperature on one
    part of the sky so close to the temperature on the other side of the sky? Why is the universe
    so flat?


    To fit observations, $\lcdm$ requires initial conditions with a particular set of properties.
    Some of these properties are surprising, and require an explanation.
    Since $a(\tau)\propto \tau$ during the radiation dominated era we can calculate
    the total conformal distance a photon could have traveled between the singularity at $a_i=0$
    and recombination at $a_{rec}=1/1100$. We find
    \begin{align}
        \chi_{rec} = \int_{0}^{\tau_{rec}} d \tau = \int_{0}^{a_{rec}} \frac{d\tau}{da}da \propto a_{rec}.
    \end{align}
    We can see that this is finite.
    Including the prefactor we find more precisely that $\chi_{rec}=280Mpc$.
    We can also calculate that the conformal distance that a photon could have
    freely streamed through the universe since
    last scattering is $\chi_0=14000Mpc$.
    This means that we would expect the homogeneous patches at recombination to span
    at most an angle of
    \begin{align}
        \tan^{-1}\left(2\frac{\chi_{rec}-0}{\chi_{0}-\chi_{rec}}\right) \approx 2^{\circ}
    \end{align}
    on the $\cmb$ sky today. We can then calculate the number of disconnected patches
    we should see as
    \begin{align}
        \frac{4\pi(\chi_{0}-\chi_{rec})^2}{\pi(\chi_{rec}-0)^2} \approx 10^5.
    \end{align}
    This is completely at odds with observations---the $\cmb$ is in fact very close to
    homogeneous \textit{across the entire sky}, clearly a feature of our universe
    that requires an explanation.


    Photons travelling on $45'$ lines on a $\tau-x$ plot. There not being enough time for opposite
    sides of the $\cmb$ to come into thermal equilibrium. But if you change the relation between
    $t$ and $\tau$ then you extend the plot, and you do make contact. \textcolor{red}{PLOT THIS!!}


    The flatness problem is the statement that our universe is much flatter that one would expect.
    The reason we would expect otherwise, is that deviations from flatness are unstable.
    This means that if the universe is very close to flat now (as we measure it to be)
    then it must have been surprisingly close to flat at the beginning of the $\lcdm$
    evolution. A natural explanation of this would be an attractive quality in a
    theory of the very early universe.


    We will mention three more features of the $\lcdm$ initial conditions that we would
    like an explanation for. Those three features are the adiabaticity of the initial
    perturbations, the (as measured so far) Gaussianity of those perturbations, and finally
    the very slight deviation from perfect scale invariance that observations demand the
    initial power spectrum must have had.


    Adiabaticity is the statement that for each of the components of the universe, their
    initial conditions $\delta\rho_i$ were related such that
    \begin{align}
        \frac{\delta\rho_i}{\bar{\rho}'_i} = \frac{\delta\rho_j}{\bar{\rho}'_j}.
    \end{align}
    To phrase this another way, by performing a local reparametrisation of time $t\rightarrow t+\delta t(x)$
    \begin{align}
        \bar{\rho}_i(t)+\delta \rho_i(t,x) = \bar{\rho}_i(t)+\delta t(x)\bar{\rho}'_i
        = \bar{\rho}_i(t+\delta t(x))
    \end{align}
    we can absorb the perturbations in \textit{all} of the components. The implication that we can draw
    is that the initial conditions of these separate components were generated by a process that was simpler than
    it could have been.
\textcolor{red}{But entropy perturbations source adiabatic, right? So how surprising is it really?}


    The Gaussianity of the initial conditions is the statement that their statistics are completely described by
    their two-point function. We will discuss this in more detail in a later section.
    The deviation from perfect scale invariance has been measured to high confidence by the $\planck$
    satellite. \textcolor{red}{Refer to WMAP!}~\cite{Senatore_wmap_2009}. Scale invariance is the statement that
    \begin{align}
        \left<f(\mathbf{x})f(\mathbf{x'})\right> = \left<f(\lambda\mathbf{x})f(\lambda\mathbf{x'})\right>.
    \end{align}
    From this, we can see that scale invariance needs $P(k)\propto k^{-3}$.
    It turns out that observations show that this is not precisely true.
    In fact, it was found that $P(k)\propto k^{n_s-4}$, with $n_s=0.9649 \pm 0.0042$.

    \subsection{Driving inflation with a scalar field}
We will work with an inflaton action of the form
\begin{align}
S = \int d^4x \sqrt{-g}P(X,\phi)
\end{align}
with $X=-\frac{1}{2}g^{ab}\nabla_a \phi\nabla_b \phi$.
We work with the number of e-folds, $N$, as our time variable
$x'=\frac{dx}{dN}=a\frac{dx}{da}$
and recall the definitions of the slow-roll parameters~\eqref{slowrollparams},
though we make no assumption that these are actually small.
$c_s$ is the sound speed of the theory, which can vary with time:
\begin{align}
c_s=\frac{P,_X}{P,_X+2XP,_{XX}}.
\end{align}
The background quantities are evolved according to the Friedmann equations,
which are set with consistent initial conditions.
The expression for the energy density is
\begin{align}
    \rho = 2XP,_X-P,
\end{align}
so then
\begin{align}
    \rho' = -6XP,_X.
\end{align}
Then, using the Friedmann equation we obtain
\begin{align}\label{PXepsilon}
    \varepsilon &= -\frac{1}{2}{\phi'}^2P,_X.
\end{align}
The equation of motion for $\phi$ is~\cite{Hu_2011}
\begin{align}\label{phieom}
    \phi''+(3c_s^2-\varepsilon)\phi'+H^{-2}\frac{\rho_\phi}{\rho_X}=0.
\end{align}
We can now evolve $\tau_s$, $\phi$ and $H$ numerically
using~\eqref{tausdef},~\eqref{phieom} and~\eqref{PXepsilon} respectively.
In the examples we use, we set the initial conditions by prescribing $\phi_{start}$,
calculating an approximate value of $\phi'_{start, approx}$ using the slow-roll approximation,
then using $\phi'_{start, approx}$ to obtain values for $c^{start}_s$ and $H_{start}$.
Taking those values for the sound speed and Hubble parameter as exact, along with the
prescribed value for $\phi_{start}$, we can calculate the corresponding exact value
of $\phi'_{start}$, obtaining consistent initial conditions to start out numerical evolution.


See also~\cite{warp_features_dbi}
and~\cite{cmb_pol_ics} for initial conditions.



    We desire a period of exponential expansion in the early universe to explain the
    scale-invariance of the initial perturbations.
    One simple way one could imagine driving this expansion is through a single
    scalar field. This scalar field would have
    \begin{align}
        \rho_\phi &= \frac{1}{2}\dot{\phi}^2+V(\phi)\\
        P_\phi &= \frac{1}{2}\dot{\phi}^2-V(\phi).
    \end{align}
    For successful inflation with a cosmological constant, we want $w=-1$,
    i.e.\ that $V(\phi)\gg\frac{1}{2}\dot{\phi}^2$. Since the kinetic term is required to
    be small, this is referred to as \textit{slow-roll} inflation.
    The Friedmann equations then become
    \begin{align}
        H^2 &\approx \frac{8\pi G}{3}V(\phi),\\
        \frac{\ddot{a}}{a} &\approx -\frac{4\pi G}{3}\left(-2V(\phi)\right).
    \end{align}
We define the Hubble parameter and the standard ``slow-roll'' parameters:
\begin{equation}
\label{slowrollparams}
\begin{split}
    H = \frac{d\ln a}{dt}	\,,
    \qquad
    \eps &= -\frac{d\ln H}{dN}	\\
    \eta = \frac{d\ln \varepsilon}{dN}	\,,
    \qquad
    \eps_s &= +\frac{d\ln c_s}{dN}	\,.
\end{split}
\end{equation}

From the continuity equation and the Friedmann equation
\begin{align}
    &&\rho' &= -3(\rho+P)\\
    \implies&&6HH' &= -3(2X P,_{X})\\
    \implies&&-H^2\varepsilon &= -\left(\frac{1}{2}H^2{\phi'}^2\right) P,_{X}\\
    \implies&&\varepsilon &= \frac{1}{2}{\phi'}^2 P,_{X}.
\end{align}
In the canonical case this simplifies to
$\varepsilon = \frac{1}{2}{\phi'}^2$.
In the DBI case we find
$\varepsilon = \frac{1}{2}\frac{{\phi'}^2}{c_s}$.

This allows us to rephrase the Friedmann equation in the following useful
way. Note that this is still exact, the slow-roll approximation has
not been used.
\begin{align}
    &&H^2 &= \frac{1}{3}\left(\frac{\varepsilon H^2}{P,_{X}}+V(\phi)\right)\\
    \implies &&  H^2 &= \frac{V(\phi)}{3-\frac{\varepsilon}{P,_{X}}}.
\end{align}

\newpage
    \subsection{Criteria for successful inflation}
    \textcolor{red}{Rewrite entirely in terms of $a$, $H$? And move earlier.}
    For inflation to solve the horizon problem, it must result in a shrinking comoving
    Hubble radius, disconnecting regions that had previously been in thermal contact.
    This implies that
    \begin{align}
        \frac{d}{dt}\left(aH\right)^{-1} = -\frac{\ddot{a}}{\dot{a}^2} < 0
    \end{align}
    so $\ddot{a}>0$, i.e.\ the expansion is accelerating. Another way of writing this is
    \begin{align}
        \frac{d}{dt}\left(aH\right)^{-1} &= -\frac{1}{(aH)^2}\left(\dot{a}H+a\dot{H}\right)\\
            &= -\frac{1}{a}\left(1-\varepsilon\right)
    \end{align}
    and so we need $\varepsilon<1$ to successfully inflate the universe.
    This is necessary but not sufficient---we still need to generate
    the correct statistics for the primordial perturbations.
    This implies we must be close to a perfect de Sitter phase, i.e.\ $\varepsilon\ll1$.
    To match the measured deviation from scale-invariance in standard slow-roll
    inflation we would need $\varepsilon\sim O(10^{-2})$.


For a model with a canonical kinetic term,~\eqref{phieom} simplifies
to
\begin{align}
    \phi''+(3-\varepsilon)\phi'+H^{-2}V'(\phi)=0.
\end{align}
In the slow-roll approximation we take assume $\phi''\ll\phi'$,
and so
\begin{align}
    \phi'\approx\frac{V'(\phi)}{3H^2}\approx\frac{V'(\phi)}{V(\phi)}.
\end{align}
From this, we see that demanding that $\varepsilon$ be small places a constraint
on the flatness of the potential in a canonical model.


We also require $\eta\ll1$ to maintain inflation for the sufficient number
of e-folds.


\newpage
\section{Statistical observables}
    \subsection{Checking dice for fairness \textcolor{red}{RENAME}}
\textcolor{red}{Move earlier.}
    The prediction of the fundamental quantum theory is a statistical one,
    i.e.\ a prediction of the distribution from which our observation will be drawn.
    As such, we need to talk in terms of estimators, estimating how likely it
    would be to see the sky we do, assuming some fundamental theory.


    Our result will be a constraint on an inflationary parameter, stated at e.g.\ $95\%$ confidence.
    For example, if we flipped one fair coin $100$ times, we would expect an even split of heads
    and tails. Should we be suspicious of the fairness of the coin if we instead get $60$ heads
    and $40$ tails? Or $80$ heads and $20$ tails? An event at least as extreme at a $60$-$40$ split
    will occur $3.5\%$ of the time. For a $80$-$20$ split however, the probability of an event that
    extreme is around one part in $10^{10}$, effectively impossible. This means that if we observed
    a trial where a coin came up heads in $80$ out of $100$ cases, we could suspect that out hypothesis
    of fairness was incorrect.
    One can calculate that, for a fair coin, $95\%$ of the time the events will fall within the
    range $[41,59]$. We call this the $95\%$ confidence interval.
    What we wish to do is the opposite, however. Instead of calculating using known probabilities,
    we wish to take a series of observations of some random variable and use these observations
    to estimate the probability distribution that they were drawn from. Given an infinite number
    of observations this task has a straightforward solution, simply plotting the normalised
    histogram of the results. However, in the context of cosmology we will only have a finite amount
    of information available to us, and thus a limit to how well we can ever measure the relevant
    probability distributions. This limitation is known as cosmic variance.



    We quantify this expected scatter around the mean using a quantity known as the standard deviation.
    The standard deviation of a random variable $X$ is the square root of the expected value of the
    squared deviation from the mean $\mu$,
    \begin{align}
        \sigma = \sqrt{E\left[{(X-\mu)}^2\right]}.
    \end{align}
    In our example above of flipping one coin $100$ times, the total number of heads has $\mu=50$
    and $\sigma=5$.

    
    We have only one universe that we can observe, only one draw from the probability distribution
    we are trying to probe. However, there is a lot of information in that one draw.
    \textcolor{red}{Motivate and explain this all better!} This is related to the concept
    of ergodicity. This is the statement that $\left<\cdot\right>$ as an expectation over ensembles
    at a fixed point is the same as the expectation over point for a fixed ensemble.
    This assumes homogeneity, stationarity, and that distant points are uncorrelated.
    In terms of our coin example, this is related to the statement that flipping one coin
    $100$ times should probe the probability distribution in the same way as flipping
    $100$ identical coins once each.
    %Ergodicity: ``In an ergodic scenario, the average outcome of the group is the same as the average outcome of the individual over time. An example of an ergodic systems would be the outcomes of a coin toss (heads/tails). If 100 people flip a coin once or 1 person flips a coin 100 times, you get the same outcome.'' from https://taylorpearson.me/ergodicity/
    % See also https://nms.kcl.ac.uk/eugene.lim/AdvCos/lecture2.pdf


    We can define the expectation value of a function of a discrete or
    continuous random variable $x$, or a functional $F$ of a field configuration $f(x)$, respectively as
    \begin{align}
        \left<f(x)\right> &= \sum_i x_i P(x_i)\label{expectation_value_discrete}\\
        \left<f(x)\right> &= \int dx~x \rho(x)\label{expectation_value_cont}\\
        \left<F\left[f(x)\right]\right> &= \int \mathcal{D}f~F\left[f(x)\right] P\left[f(x)\right]\label{expectation_value_field}
    \end{align}
    where the sum and integral over $x$ are over the values that $x$ can take,
    and the functional integral over $f$ is over all the field configurations
    that $f(x)$ can take.


    In our example of the $100$ coin flips, we used~\eqref{expectation_value_discrete}
    to calculate the mean and standard deviation. For a continuous variable,
    like the average height of a population or the average temperature in a given room,
    we would use~\eqref{expectation_value_cont}. In this thesis, we will be working with
    the expectation value of field configurations, so we will use~\eqref{expectation_value_field}.
    \textcolor{red}{Talk about quantum to classical transition!}
    \begin{align}
        \left<\hat{v}_{\bk}\hat{v}_{\bk'}\right>
                         &= \left<0|\hat{v}_{\bk}\hat{v}_{\bk'}|0\right>\\
                         &= |v_k|^2\left<0\left|\left[\hat{a}_\bk,\hat{a}_{-\bk}^{\dagger}\right]\right|0\right>\\
                         &= |v_k|^2\delta(\bk+\bk')\\
                         &= P_{v}(k)\delta(\bk+\bk')
    \end{align}


    \subsection{Power spectra}
    Cite~\cite{Planck_inflation_2015, Planck_inflation_2018}.
    The power spectrum and other correlations
    are predicted by inflation. 
    Define n-point correlations, their Fourier transforms, talk about them as observables.
    

    A model of inflation will predict the statistical properties of the distribution of matter
    that forms the initial conditions for the $\lcdm$ evolution of our universe.
    From this, we can calculate the statistical properties of the $\cmb$ sky that we see.
    One such property is the two-point correlation of the temperature $\phi$
    at a point $x$ on the sky, $\left<\phi(x)\phi(x')\right>$. When there is a large
    angular separation between $x$ and $x'$ \textcolor{red}{talk about cosmic variance
    of different modes.}

\section{Outline of thesis}
\textcolor{red}{Should some of this be moved to the end of chapter 2? Maybe a high-level outline here
and then a technical outline at the end of chapter 2.
Remove numbered list, turn into paragraphs?}
    \subsection{Goals \textcolor{red}{MERGE, REMOVE SUBTITLES}}
    \begin{enumerate}
        \item Connecting inflation models directly to observations,
            through the bispectrum.
        \item Constraining the parameters of inflation models, not phenomenological templates and $f_{NL}$.
        \item To obtain the full shape information, not point samples or a limit.
        \item Efficient numerics gives access to more accurate, and in some cases new, feature shapes.
    \end{enumerate}
    \subsection{Methods}
    \begin{enumerate}
        \item Building separability into the tree-level in-in formalism.
        \item The CMB calculation~\cite{Sohn_2021}: expensive, but need only be done once per primordial basis.
        \item So, we want a basis expansion that converges quickly for a broad range of inflation models.
        \item Convergence on the cube is different to the tetrapyd.
        \item Turns out to be much faster at primordial level than previous numerical methods
            (as it in a sense converges way faster, and as it enables us to use faster numerical methods than otherwise).
    \end{enumerate}
    \subsection{Results}
    \begin{enumerate}
        \item First development/implementation of the formalism for calculating the expansion to high orders.
        \item We recognised and described the central issue of the cube vs tetra problem.
        \item Found a basis with broad descriptive power (and other less powerful basis sets).
        \item This allowed the first validation of these methods on features.
        \item Connect to CMB, get constraints on DBI $c_s$.
    \end{enumerate}

The thesis is organised as follows. In chapter~\ref{chapter:intro_bispectra} we present brief reviews
of the various parts of the pipeline that connects inflation scenarios to observations
through the bispectrum.
We review the usual paradigm of bispectrum estimation in the CMB,
and the motivation for separable bispectra. We review the in-in formalism,
for calculating the tree level bispectrum for a given model of inflation.
We review $P(X,\phi)$ models of inflation as an example, and
some of the usual approximate bispectrum templates
that we aim to bypass.
We will draw our validation scenarios from these models.
We discuss previous numerical codes for
calculating the primordial bispectrum $k$-configuration by $k$-configuration,
which contrasts our separable basis expansion.
We review the previous work in achieving separability through modal expansions
in~\cite{Funakoshi},
and we discuss methods of testing
numerical bispectrum results, defining our relative difference measurement.
In chapter~\ref{chapter:methods} we present our methods.
Since the paradigm we aim to present is only viable if we can find a basis
that can efficiently represent a wide variety of bispectra,
we begin with this distinct and separate, but nonetheless vital, discussion.
We discuss the effects of the
non-physical $k$-configurations on the convergence of our expansion on
the tetrapyd, and present an efficient basis.
Then, we recast the usual in-in calculation into an explicitly separable form,
in terms of an expansion in an arbitrary basis,
and detail our methods for carefully calculating the coefficients to high order.
In section~\ref{sec:validation} we validate our methods and implementation
on inflation scenarios with varied features from the literature,
and we finish with a discussion of future work in chapter~\ref{chapter:conclusion}.

