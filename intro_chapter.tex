%\input{commands}
\chapter{Introduction I: Cosmology}\label{chapter:intro_general}
\section{The standard cosmological history}\label{sec:general_intro}
\textcolor{red}{Rewrite introduction to be more focused on topic of thesis.
Review the state of the art. Main obstacles we aim to address. Main results.
Overview for typical reader familiar with cosmology. Basically a more comprehensive
and detailed abstract.}
%\subsection{The bispectrum}
    \subsection{Fundamentals}
When we look out into the sky, we see light that has travelled for a long time to get to us.
In that light we see images of the universe as it was when the light left its source.
We see the moon as it was a second ago, or the Andromeda Galaxy as it was millions of
years ago. To what extreme can we push this? How far back in time can we see? As we look further
away, and further back in time, we see a universe that looks nothing like the one we know today.
We see a universe that was extremely hot and dense, but also extremely uniform. Today
we have distinct galaxies and the empty spaces in between, but looking far enough back we see a universe
without that structure, instead simply uniformly hot and dense everywhere.


But this uniformity wasn't perfect. There were patches that were very slightly more dense than the average,
and others that were very slightly less dense. These non-uniformities provided the seeds that would form
structure---the over-dense regions had a slightly higher gravitational pull on their surroundings,
and caused the nearby matter to begin to fall in. This process would eventually result in the structure
we see in the universe today: the stars, the galaxies, and all the complexity on our own planet.


The precise details of this story have been fleshed out thanks to the work of many people.
It is incredible that we can say so much about the radically different epochs of the universe that came before our own,
between this hot, dense initial state and today.
More work remains to be done, however---one aspect we can question is the origin of the non-uniformities
that provided the seeds of structure. What was the physics that generated them?
The goal of the work described in this thesis is to develop methods to connect this physics
to the sky we see today; more precisely, we aim to use the light from
the early universe, the Cosmic Microwave Background ($\cmb$),
to constrain physical theories of the very early universe (the epoch of exponential
expansion known as inflation) using a particular
statistical description, known as the bispectrum.
We aim to mature the connection between models of inflation and their predicted bispectra, so we can better
extract the fundamental physics waiting to be found in the universe's youth.


    We begin with a brief discussion of the fundamentals of cosmology.
    General relativity is the theory which describes how the evolution of the universe depends
    on its matter and energy content. It describes the path of light as a geodesic,
    the generalisation of a straight line to curved spacetime.
    The equations that we derive from this theory (the Einstein equations) are written in terms of a metric,
    which we denote by $g_{\mu\nu}$. This object, a symmetric rank-two tensor,
    defines our notion of distance, and thus encodes
    the causal structure of the universe---which events can influence which other events.
    The Einstein equations are
    \begin{align}\label{einstein_equations}
        G_{\mu\nu} = 8\pi G T_{\mu\nu}
    \end{align}
    where $G_{\mu\nu}$ is a complicated function of $g_{\mu\nu}$ which
    will be presented in~\eqref{einstein_tensor},
    $T_{\mu\nu}$ is a symmetric rank-two tensor that describes the matter
    and energy content of the universe,
    and $G$ is Newton's gravitational constant.


    Spatially homogeneous and isotropic universes (i.e.\ ones that at some given
    time, look the same everywhere and in every direction)
    define a class of solutions of the Einstein equations.
    The metric of such a homogeneous and isotropic universe
    is known as a Friedmann–Lemaître–Robertson–Walker metric
    (or more usually, an FLRW metric). In this context
    the Einstein equations reduce to the Friedmann equations, which we will soon present
    in~\eqref{friedmann_1}.
    But first, we present the FLRW metric itself\footnote{
        In this thesis we will always set the speed of light to unity.
    }
    \begin{align}\label{flrw_metric}
        ds^2 = g_{\mu\nu}dx^{\mu}dx^{\nu} = - dt^2 + a(t)^2 dS_3^2,
    \end{align}
    where $a(t)$ has the interpretation as the scale factor
    which describes the evolution of this homogeneous and isotropic universe.
    The line element $ds^2$ then determines the interval between two spacetime points.
    Defining conformal time $\tau$ such that
    \begin{align}\label{conformal_time_defn}
        \frac{d\tau}{dt} = \frac{1}{a},
    \end{align}
    we can then write the metric in the useful form
    \begin{align}\label{flrw_metric_tau}
        ds^2 = a(t)^2\left(-d\tau^2 + dS_3^2\right).
    \end{align}
    %Or, more intuitively, it determines the proper time measured by an observer who freely falls
    %between two events to be the integral of $d\tau=\sqrt{-g_{\mu\nu}dx^\mu dx^\nu}$.
    The homogeneity, isotropy and curvature of space are encoded in $dS^2_3$,
    but a non-trivial time dependence of $a(t)$ breaks homogeneity in time.
    We will only consider a flat universe, so $dS_3^2=dx^2+dy^2+dz^2$,
    which corresponds to Euclidean geometry.
    This makes~\eqref{flrw_metric_tau} a function of time multiplied by the Minkowski metric,
    in conformal time.
    The spatial part of the metric is written in terms of comoving distances, which
    do not change with the expansion of the universe---to convert a comoving distance $r$
    to a physical distance $r_{phys}$ one simply multiplies by the scale factor $r_{phys}=a(t)r$.
    This means we can calculate the physical velocity
    \begin{align}
        v_{phys} = \frac{dr_{phys}}{dt} = a(t)\frac{dr}{dt}+Hr_{phys}.
    \end{align}
    The result is a combination of the motion of an object with respect to the comoving coordinates
    $\left(a(t)\frac{dr}{dt}\right)$ and a term due to the expansion of the universe, which we see is linear
    in $r_{phys}$ with a (time-dependent) coefficient $H$.


    The local curvature of spacetime and its evolution is determined by the matter and energy
    present in the universe.
    Radiation is one of the cosmologically important components of the present universe, in the form of the
    photons in the cosmic microwave background, the $\cmb$.
    Today these photons free-stream through the universe.
    Another component is matter, the primary part of which
    is \textit{dark} matter.
    This is invisible to our telescopes (which depend on electromagnetic radiation) but can be mapped by the
    effect its mass has on the curvature of spacetime, which we see through
    effects such as the lensing of passing photons.
    A more familiar component is what cosmologists refer to as baryonic matter, which is
    the protons, electrons, neutrons and all the other particles which make up the visible galaxies, stars
    and humans.
    The last component of our universe that we will mention is dark energy.
    At present, dark energy makes up the largest contribution to the
    energy budget of the universe.
    We will take dark energy to contribute to the energy content of the
    universe through a cosmological constant which does not
    dilute as the universe expands.
    This will be encoded in~\eqref{einstein_equations}
    through the stress-energy tensor $T_{\mu\nu}=\mpl^2\Lambda g_{\mu\nu}$\footnote{
        $\mpl$ is the reduced Planck mass, $\mpl^2=\frac{1}{8\pi G}$,
        in a system of units with $c=\hbar=1$.
        In practice we will usually also set $\mpl=1$.
    }.
    The radiation is denoted by $\gamma$,
    the matter by $m$, and the cosmological constant by $\Lambda$.


    The components of the matter and energy content of the universe enter~\eqref{einstein_equations} through
    their energy-momentum tensors $T^{\mu\nu}$, which in a homogeneous and isotropic universe
    we can approximate as that of a perfect fluid. Labeling the fluid $X$,
    \begin{align}\label{stress_tensor}
        T_X^{\mu\nu} = (\rho_X+p_X)U_X^\mu U_X^\nu+p_Xg^{\mu\nu},
    \end{align}
    where the index structure ensures the fluid is isotropic in its rest frame.
    $U_X^\mu$ is the four-velocity of the fluid, with $U_X^0=1$ and $U_X^i=0$
    in the rest frame.
    These fluids are completely characterised by the time dependence
    of their rest-frame energy density $\rho_X(t)$ and their rest-frame
    pressure density $p_X(t)$,
    both of which have no dependence on position, to preserve the homogeneity of the solution.
    For the components of the universe that we wish to describe,
    taking the quantity $w_X=\frac{p_X}{\rho_X}$ as constant will be a good description.
    This equation of state quantity $w$ takes the values $0$, $1/3$ and $-1$ for matter, radiation
    and the cosmological constant respectively.


    Fortunately, when $g_{\mu\nu}$ takes the form~\eqref{flrw_metric},
    the equations~\eqref{einstein_equations} simplify to the much simpler Friedmann equations
    %\footnote{
    %    In~\eqref{einstein_equations} and~\eqref{friedmann_1} we have temporarily reinstated $8\pi G$.}
    \begin{align}\label{friedmann_1}
        H^2 &= \frac{8\pi G \rho}{3},\\
        \frac{\ddot{a}}{a} &= -\frac{4\pi G}{3}\left(\rho+3p\right),
    \end{align}
    where $H=\frac{\dot{a}}{a}$, and $\rho$ and $p$ are respectively the sum of the
    energy densities and of the pressure densities of the
    components of the universe.
    %Note that we will usually set $8\pi G=\mpl^{-2}=1$.
    We use a dot to refer to a derivative with respect to coordinate time $t$.
    We also have the continuity equation
    \begin{align}\label{continuity_equation}
        \dot{\rho}_X + 3\frac{\dot{a}}{a}\left(\rho_X+p_X\right) &= 0.
    \end{align}
    This describes
    the effect the expansion of the universe has on the energy density of the fluid,
    and can be shown to hold for each component
    separately using the first law of thermodynamics for an adiabatic process.

    Given the equations of state and the densities of the different
    components of the universe, one can then calculate the time dependence of the
    scale factor $a(t)$ and of the densities $\rho_X$ and $p_X$ of each component $X$.
    In particular, using~\eqref{continuity_equation} and $\rho=wp$
    we find
    \begin{align}\label{rho_a_dep_X}
        \rho_X\propto a^{-3(1+w_X)}
    \end{align}
    for each component $X$, which can then be used with~\eqref{friedmann_1}
    to determine the time dependence of the scale factor
    $a(t)$. This is summarised in Table~\ref{lcdm_dep_table}.
    This gives us the homogeneous background evolution of the universe---evolution
    which has observable effects in physical phenomena
    such as red-shifted photons arriving from distant galaxies.


    As we have mentioned, free particles travel on geodesics.
    This means that they follow the path which extremises the proper time,
    the time experienced by an observer travelling along with the particle.
    This principle allows the derivation of
    the geodesic equation
    \begin{align}\label{geodesic_eqn_1}
        %\frac{dP^\mu}{d\lambda}+\Gamma^\mu_{\nu\sigma}P^\nu P^\sigma=0
        P^\nu\partial_\nu P^\mu+\Gamma^\mu_{\nu\sigma}P^\nu P^\sigma=0
    \end{align}
    where $P^\mu$ is the particle's $4$-momentum.
    The Christoffel symbols $\Gamma^\mu_{\nu\sigma}$ are functions of the
    metric, and are used to build $G_{\mu\nu}$, which appeared in the Einstein equations~\eqref{einstein_equations}
    \begin{align}
        \Gamma^\mu_{\nu\sigma} &= \frac{1}{2}g^{\mu\gamma}\left(
        \frac{\partial g_{\gamma\nu}}{\partial x^{\sigma}}
        +\frac{\partial g_{\gamma\sigma}}{\partial x^{\nu}}
        -\frac{\partial g_{\nu\sigma}}{\partial x^{\gamma}}
        \right)\label{christoffel_symbols}\\
        R_{\mu \vu} &= \partial_\mu \Gamma^\mu_{\vu \sigma}-\partial_j\Gamma^\mu_{\sigma\mu}+
        \Gamma^\mu_{\mu \tau}\Gamma^\tau_{\vu \sigma}-\Gamma^\mu_{\vu \tau}\Gamma^\tau_{\mu \sigma}\label{ricci_tensor}\\
        G_{\mu\nu} &= R_{\mu\nu} - \frac{1}{2}g_{\mu\nu}R\label{einstein_tensor}
    \end{align}
    where $R_{\mu \nu}$ is known as the Ricci curvature tensor, and $R$ is its trace.
    By homogeneity and isotropy,~\eqref{geodesic_eqn_1} can be rewritten as
    \begin{align}
        P^{0}\frac{\partial P^\mu}{\partial t}+\Gamma^\mu_{\nu\sigma}P^\nu P^\sigma=0.
    \end{align}
    The magnitude of the physical $3$-momentum is $p^2\equiv a^2\delta_{ij}P^i P^j$.
    Evaluating~\eqref{christoffel_symbols} for the metric~\eqref{flrw_metric}
    and using $P_\mu P^\mu=-m^2\implies EdE=pdp$
    we get that
    \begin{align}\label{eq:redshifting}
        p\propto \frac{1}{a}.
    \end{align}
    The interpretation of this result is that as the universe expands,
    the particle's momentum decreases, in a manner determined by the
    time dependence of $a(t)$.
    For a photon,
    this implies that its energy decreases and its wavelength increases
    as it free streams through the universe.
    %The geodesic equation~\eqref{geodesic_eqn_1} encodes many other physical effects when applied
    %to the evolution of perturbations around the homogeneous background
    %(i.e.\, allowing $g_{\mu\nu}$ to deviate from homogeneity)
    %allowing us to understand how the statistics of the perturbations at the
    %start of the $\lcdm$ evolution evolved into the perturbations we see today.


    Using~\eqref{continuity_equation} and the equation of state ($w$, summarised in Table~\ref{lcdm_dep_table})
    for matter, radiation and the cosmological constant $\Lambda$
    we can solve for the dependence of $\rho_X$ on $a$, as we saw in~\eqref{rho_a_dep_X}.
    For matter, we obtain the simple result that $\rho_m\propto a^{-3}$,
    i.e.\ that it dilutes with the expansion of the universe.
    For radiation we find $\rho_r\propto a^{-4}$,
    i.e.\ that it dilutes with the expansion of the universe,
    and also loses energy as it redshifts, as we
    saw in~\eqref{eq:redshifting}.
    For $\Lambda$, we find $\rho_\Lambda\propto a^0$, by design.


    \begin{table}[h!]
    \begin{center}
        \begin{tabular}{ c c c c }
            Epoch & $w$ & $a(t)$ & $a(\tau)$ \\ 
            \toprule
            Radiation & $\frac{1}{3}$ & $t^{\frac{1}{2}}$ & $\tau$ \\
            Matter & $0$ & $t^{\frac{2}{3}}$ & $\tau^2$ \\
            $\Lambda$ & $-1$ & $e^{Ht}$ & $-\frac{1}{\tau}$
        \end{tabular}\caption{
            How the scale factor, $a(t)$, evolves in the
            different epochs of the universe.
        }\label{lcdm_dep_table}
    \end{center}
    \end{table}


    A subscript of $0$ will denote a quantity evaluated today.
    The critical density $\rho_{crit}\equiv\frac{3H^2}{8\pi G}$ is defined as
    as the (time-dependent) density for which the universe is flat.
    Using this,
    and choosing to scale our spatial coordinates such that $a_0=1$,
    we can rewrite the Friedmann equation~\eqref{friedmann_1} as
    \begin{align}\label{friedmann_omega}
        H^2 = H_0^2\left(\frac{\Omega_{r,0}}{a^4}+\frac{\Omega_{m,0}}{a^3}+\Omega_{\Lambda}\right)
    \end{align}
    where we define the fractional density
    \begin{align}
        \Omega_{X} = \frac{\rho_X}{\rho_{crit,0}}
    \end{align}
    for $X$ being radiation, matter or the cosmological constant.
    For our flat universe $\sum\Omega_{i,0}=1$.
    The measured values for these quantities and their $68\%$ limits,
    as presented in~\cite{Planck_parameters_2018}, are
    \begin{align}\label{measured_params}
        \Omega_{m,0} &= 0.3111 \pm 0.0056,\\
        \Omega_{\Lambda} &=  0.6889 \pm 0.0056.
    \end{align}
    The temperature of the blackbody $\cmb$ radiation has also been
    found to be $T_{\gamma,0}=2.725\pm0.001K$~\cite{Fixsen_2009}.
    %from which
    %we can calculate $\Omega_{\gamma,0}=9.4\times10^{-5}$.
    %Since
    %$\rho = \frac{\pi^2}{15}T^4$
    %for photons, and $T_{\gamma}=2.725\pm0.001K$
    %we have that $\rho_{\gamma}=2.02\times 10^{-15} eV^4$
    %and $\rho_{crit}=9\times 10^{-27} kg~m^{-3}$.
    %Therefore, $\Omega_{\gamma,0}h^2=2.47\times 10^{-5}$.


    Given these numbers, and~\eqref{friedmann_omega}, we can make some statements
    about the past, present and future of the universe.
    The numerical evolution of the components is illustrated in Figure~\ref{fig:lcdm_components},
    with the recent past highlighted in Figure~\ref{fig:lcdm_components_linear}.
    Firstly, the present---we see that the cosmological constant,
    $\Lambda$, is dominant now. We also see that the energy density of radiation
    is very small, $\Omega_{\gamma,0}\ll\Omega_{m,0}$.
    Looking to the past, we see that the transition from matter domination
    to $\Lambda$ domination happened relatively recently in the history of the universe,
    at $a_{m\Lambda}=\left(\frac{\Omega_{m,0}}{\Omega_{\Lambda}}\right)^{\frac{1}{3}}\approx0.77$.
    We can see that this domination will continue as the matter and radiation content of the
    universe dilute and redshift away.
    Running the clock backwards, that is towards $a=0$, we see that the matter and radiation had
    higher energy densities in the early universe, which is due to their scaling as $a^{-3}$ and $a^{-4}$
    respectively.
    Running the clock back far enough, we see
    that despite $\Omega_{\gamma,0}\ll\Omega_{m,0}$ today, at one stage it was the
    case that $\Omega_{\gamma}\gg\Omega_{m}$, and the universe was dominated by radiation.
    This is due to the relative scaling $\frac{\Omega_{\gamma}}{\Omega_{m}}\propto a^{-1}$.
    This epoch is the limit on how far back in time we can see---before this point,
    photons were unable to free stream.
\begin{figure}[h!]
\centering     %%% not \center
    \includegraphics[width=.75\columnwidth]{plots/lcdm_components.png}
\caption{
    The evolution of the components of the universe up to the present, and slightly beyond.
    The vertical grey lines mark matter-radiation equality, matter-dark energy equality,
    and the present day, respectively.
    %We measure these quantities at the present, and
    %extrapolate into the past and future using the Friedmann equation~\eqref{friedmann_omega}.
    These quantities are evolved using~\eqref{friedmann_omega} and~\eqref{continuity_equation}.
    In the past, densities of matter and radiation were
    far higher, and the radiation energy density dominated over the matter.
    During this high density epoch, the expansion of the universe ($H(t)$) was far stronger.
    In the future, as $\Lambda$ comes to dominate, $H(t)$ will become constant.
}\label{fig:lcdm_components}
\end{figure}
\begin{figure}[h!]
\centering     %%% not \center
    \includegraphics[width=.75\columnwidth]{plots/lcdm_components_linear.png}
\caption{
    The evolution of the components of the universe, zoomed in to more clearly
    show the matter-dark energy transition. The first vertical grey line is
    matter-dark energy equality, the second is the present day.
}\label{fig:lcdm_components_linear}
\end{figure}

    %In that early stage of the universe, the temperature was high enough that the photons
    %were tightly coupled to the baryons. The photon pressure supported pressure waves in that
    %nearly-homogeneous fluid, and the coupling meant that the baryons were dragged along with those
    %waves. Once the temperature and density dropped sufficiently that this was no longer the case,
    %the baryons were dropped, and instead began to fall back into the gravitational wells that the dark matter
    %had meanwhile been forming. The imprints of those pressure waves is thus seen in the positions
    %of galaxies today (in the statistics of their separations) and in the acoustic oscillations of the $\cmb$.


    The standard cosmological model of the history of the universe,
    the $\lcdm$ ($\Lambda$, cold dark matter) model, takes this
    era of radiation domination as its starting point.
    In the distant past, the components of the universe (all the different types of matter and radiation)
    were in thermal equilibrium with each other---that is, they were interacting with each other,
    and the interaction was sufficiently efficient that their temperatures matched
    and the net flow of thermal energy between them was zero.
    At sufficiently early times, the density of free electrons
    was so high that the mean free path of photons was very short. Once the temperature
    dropped enough that neutral atoms formed, the mean free path of photons became long
    enough that they could travel cosmological distances, and eventually fall into our telescopes---this
    is the origin of the $\cmb$.


    The evidence for this story is strong. It includes observable
    predictions for the relative abundances of the lightest
    elements~\cite{peebles_abundances} (which were forged in the first few minutes of the universe)
    and baryon acoustic oscillations~\cite{peebles_bao} in the $\cmb$ and large scale structure ($\lss$).
    The latter is not evidence for thermal equilibrium,
    but can be directly linked to the end of the coupling
    between the baryons and the acoustic waves in the radiation,
    before the release of the $\cmb$.
    %so densely packed with radiation that it could
    %support acoustic pressure waves, and a universe that had dropped below that density.
    There are however unanswered questions within the $\lcdm$ model.
    One is the Hubble tension, an apparent tension between different measurements of the
    precise rate of expansion of the universe today---see for example~\cite{tensions_2019, Freedman_2021}.
    Despite the remaining questions, it is incredible that only a century ago the cutting edge debate was
    whether the universe we know had a beginning or not---compared to
    now, where the debate rages over the second and
    third significant figures of its age.


    Before focusing our whole attention on the past in the rest of this thesis, let us briefly
    look toward the future of the universe.
    What will we see as the universe evolves?
    Eventually $a$ will be
    sufficiently large that $\Omega_{\Lambda}$
    will the dominant contribution to the energy budget of the universe.
    The beginning of this epoch can be seen in Figure~\ref{fig:lcdm_a} and Figure~\ref{fig:lcdm_adot},
    which show the evolution of $a(t)$ and $\adot(t)$.
\begin{figure}[h!]
\centering     %%% not \center
    \includegraphics[width=.75\columnwidth]{plots/lcdm_a.png}
\caption{
    The evolution of the scale factor. For most of the $\lcdm$ history
    it evolves as some power of $t$ (see Table~\ref{lcdm_dep_table}),
    however as $\Lambda$ comes to dominate
    it will begin to grow exponentially.
    The vertical grey lines mark matter-radiation equality, matter-dark energy equality,
    and the present day, respectively.
}\label{fig:lcdm_a}
\end{figure}
\begin{figure}[h!]
\centering     %%% not \center
    \includegraphics[width=.75\columnwidth]{plots/lcdm_adot.png}
\caption{
    During the radiation and matter dominated eras, the evolution of
    $a(t)$ as been slowing---this is decelerating expansion.
    However, as $\Lambda$ comes to dominate,
    $\dot{a}$ will begin to increase, and the universe will enter
    an epoch of accelerated expansion.
    The vertical grey lines mark matter-radiation equality, matter-dark energy equality,
    and the present day, respectively.
}\label{fig:lcdm_adot}
\end{figure}


    In this $\Lambda$ dominated epoch we can use~\eqref{friedmann_1}
    to find
    \begin{align}
        H^2 &= H_0^2\Omega_{\Lambda}\\
        \implies \dot{a} &= \pm H_0\sqrt{\Omega_{\Lambda}}a
    \end{align}
    from which the initial conditions pick out the exponentially expanding solution,\\
    ${a(t)=a_0e^{H_0\sqrt{\Omega_{\Lambda}}\left(t-t_0\right)}}$. Using the Friedmann equation
    for sufficiently far in the future we can rewrite this as
    \begin{align}
        a(t)=a_0e^{H\left(t-t_0\right)}
    \end{align}
    where $H$ is a constant, having frozen in to a factor $\sqrt{\Omega_{\Lambda}}$ smaller than
    its value today.
    One question we can ask, to obtain some physical meaning for this evolution,
    is how far light will travel in the lifetime of the universe.
    Since $ds^2=0$ for a photon, in terms of conformal time $\tau$
    we find that for the photon travelling in the $x$ direction, $dx=d\tau$.
    This means that on a plot of $x$ vs. $\tau$, the path of a photon
    will be a line with a slope of unity, even in an expanding universe.
    Integrating, we then find that
    \begin{align}
        \tau(t)=x(t)=(Ha_0)^{-1}\left(1-e^{-H(t-t_0)}\right)+\tau_0
    \end{align}
    where as usual a $0$ subscript denotes a quantity evaluated today.
    We see that the comoving distance the photon will travel is finite, even though the physical
    distance (and the time taken, $\int dt$) diverges.
    Numerically integrating~\eqref{friedmann_omega}
    (including radiation, matter and $\Lambda$) we obtain the full evolution of $\tau$,
    illustrated in Figure~\ref{fig:lcdm_tau}.
    We find that the points that we see in today's $\cmb$ are a comoving distance of $14\Gpc$ away.
    Since $a_0=1$, the present physical distance to these points is equal to the comoving distance,
    but this will increase exponentially in the future.
    We find the asymptotic value of $\tau$ to be $19\Gpc$, indicating that we will never see the
    light that was emitted from the $\cmb$ at comoving distances greater than this.
\begin{figure}[h!]
\centering     %%% not \center
    \includegraphics[width=.75\columnwidth]{plots/lcdm_tau.png}
\caption{
    The evolution of the conformal time $\tau$ during $\lcdm$.
    We see that
    due to the eventual $\Lambda$ dominance,
    $\tau$ will asymptote to a constant,
    denoted here by the horizontal grey line at $\tau=\tau_0+(H_0a_0)^{-1}$.
    %Thus, without inflation, there is a maximum scale on which we would expect the
    %$\cmb$ to be correlated, and 
    Thus, there is a maximum comoving distance that we can ever
    expect to receive $\cmb$ photons from.
    The vertical grey lines mark matter-radiation equality, matter-dark energy equality,
    and the present day, respectively.
}\label{fig:lcdm_tau}
\end{figure}


%\begin{figure}[!pth]
%\centering     %%% not \center
%    \includegraphics[width=.75\columnwidth]{plots/lcdm_z.png}
%\caption{
%    The correspondence between redshift $z$ and $t$.
%}\label{fig:lcdm_z}
%\end{figure}


\section{Initial conditions for $\lcdm$}
    \subsection{Motivations for inflation}
    %The $\lcdm$ model does not posit a ``Big Bang singularity''---instead, as we have discussed, it
    %posits an early epoch in which the components that we have discussed are in thermal equilibrium,
    %at an incredibly hot temperature.
    %However, if one assumes that those components are all there are, and runs the equations backwards,
    %one reaches $a=0$ in finite time.
    The $\lcdm$ model relies upon an early epoch in which the components that
    we have discussed are in thermal equilibrium,
    at an incredibly high temperature.
    These components were distributed almost perfectly uniformly, but with some tiny perturbations.
    This simple scenario then evolves under the expansion of the universe
    and the gravitational collapse of perturbations (and other interactions) and forms the universe we know.
    These special characteristics of this early epoch
    then become clues to the epoch of the universe that preceded $\lcdm$.
    %Is this story of $\lcdm$ enough to explain all the features of the
    %universe that we observe?
    %It turns out that the answer is yes, if the initial conditions for the $\lcdm$
    %model have certain special characteristics.
    %This story of $\lcdm$ explains many features of the
    %universe that we observe, given that 


    One such clue that we have is known as the horizon problem.
    Roughly speaking, it is the problem that the universe is more homogeneous on
    large scales than we would expect, given the history of $\lcdm$. Why is the temperature on one
    side of the sky so close to the temperature on the other side of the sky?


    During the matter dominated era, we can take the pressure density to vanish, and
    therefore find $a(\tau)\propto \tau^2$. Therefore we can approximate
    the total comoving distance a photon could have traveled between the singularity at $a_i=0$
    and recombination at $a_{rec}=1/1100$. We find
    \begin{align}
        x_{rec} = \int_{0}^{\tau_{rec}} d \tau = \int_{0}^{a_{rec}} \frac{d\tau}{da}da \propto \sqrt{a_{rec}}.
    \end{align}
    We can see that this is finite.
    Including the radiation dominated period does not change this conclusion---performing
    the integration numerically we find the comoving distance $x_{rec}=280\Mpc$.
    % Compared to the physical = 278Mpc/1100 = 310kpc.
    We can also calculate that the comoving distance that a photon could have
    freely streamed through the universe since
    last scattering is $x_0=14000\Mpc$. This matches the physical distance
    since $a_0=1$.
    This means that we would expect the homogeneous patches at recombination to span
    at most an angle of
    \begin{align}
        \tan^{-1}\left(\frac{2x_{rec}}{x_{0}-x_{rec}}\right) \approx 2^{\circ}
    \end{align}
    % 2.57536581980 ?
    % 2.34
    on the $\cmb$ sky today.
    % Slight disagreement with Weinberg. I think
    % due to neglecting radiation and dark energy, and/or the factor of 2.
    We can also obtain an order of magnitude approximation of the number of disconnected patches
    we should see, by simply dividing the area of the $\cmb$ sky by the apparent
    area that could be causally affected by a single point at $a=0$
    \begin{align}
        \frac{4\pi(x_{0}-x_{rec})^2}{\pi(x_{rec}-0)^2} \approx 10^5.
    \end{align}
    This is completely at odds with observations---the $\cmb$ is in fact very close to
    homogeneous \textit{across the entire sky}, clearly a feature of our universe
    that requires an explanation.


    %Photons travelling on $45'$ lines on a $\tau-x$ plot.
    While in the basic $\lcdm$ model there was not enough conformal time since $a=0$ for
    opposite sides of the $\cmb$ to come into thermal equilibrium, this can be remedied by
    adding an epoch that causes the horizon (the characteristic conformal length scale $(aH)^{-1}$)
    to shrink---this would give large scales enough time
    to come into thermal equilibrium, before they leave the horizon. The usual $\lcdm$ evolution
    would then come into play, bringing these scales back inside the horizon where we can observe them.
    One way of doing this is via an effective cosmological constant---in that case the horizon $1/(aH)$ shrinks
    exponentially. We then find that $\tau=-1/(aH)$, so in the limit $a\rightarrow 0$ we see there is
    infinite conformal time in the past, to allow the entire observable universe to come into
    causal contact. We refer to this period as the epoch of inflation.


    %The flatness problem is the statement that our universe is much flatter that one would expect.
    %In~\eqref{flrw_metric}, the possibility exists that $dS_3^2$ is not the simple flat form we
    %see observationally.
    %One can write down this curvature as a contribution on the right hand side of~\eqref{friedmann_omega},
    %taking the form $\Omega_K=\frac{-K}{a^2H_0^2}$.
    %One then observes that this relative contribution would be expected to increase with time,
    %as it drops off more slowly than either matter or radiation.
    %This means that if the universe is very close to flat now (as we measure it to be)
    %then it must have been surprisingly close to flat at the beginning of the $\lcdm$
    %evolution. A natural explanation of this would be an attractive quality in a
    %theory of the very early universe.


    We will mention three more features of the $\lcdm$ initial conditions that we would
    like an explanation for. We have mentioned that the structure in the universe today
    grew from the slight deviations from homogeneity in very early universe---these three
    features are properties of those perturbations.
    Firstly, adiabaticity is the statement that for each of the components (denoted here by $i$) of the universe,
    whose energy densities we can write as
    $\rho_i(\vecx,t) = \bar{\rho}_i(t)+\delta\rho_i(t,\vecx)$, their
    initial background values $\bar{\rho}_i$ and their
    initial perturbations $\delta\rho_i$ were related such that
    \begin{align}
        \frac{\delta\rho_i}{\dot{\bar{\rho}}_i} = \frac{\delta\rho_j}{\dot{\bar{\rho}}_j}.
    \end{align}
    To phrase this another way, we can choose a local reparametrisation of time $t\rightarrow t+\delta t(x)$
    with $\delta t(x)=\frac{\delta\rho_i}{\dot{\bar{\rho}}_i}$, so that
    \begin{align}
        \bar{\rho}_i(t)+\delta \rho_i(t,x) = \bar{\rho}_i(t)+\delta t(x)\dot{\bar{\rho}}_i
        \approx \bar{\rho}_i(t+\delta t(x))
    \end{align}
    and thus the reparametrisation can absorb the linear perturbations in
    \textit{all} of the components.
    The initial perturbations in our universe appear to have obeyed this property---an
    implication we can draw from this is that a single degree of freedom
    would be sufficient to lay down these initial perturbations.


    The Gaussianity of the initial conditions is the statement that their statistics are completely described by
    their two-point correlations. We will define this, and discuss the topic in more detail
    in Section~\ref{corr_functions}. In that section we will also define scale invariance,
    the statement that correlations do not depend on the scale at which they are measured.
    It turns out that observations show that the $\cmb$ appears close to Gaussian, although
    scale invariance is not precisely respected.
    This deviation from perfect scale invariance has been measured to high confidence by the $\planck$
    satellite, with
    earlier measurements made by the WMAP satellite~\cite{Senatore_wmap_2009}.
    These are some of the properties of the early universe that the epoch of inflation
    must explain.


    \subsection{Driving inflation with a scalar field}
    These features of the early universe can be explained by a period of near-exponential expansion
    preceding the usual $\lcdm$ evolution.
    One simple way one could imagine driving this expansion is through a single
    scalar field.
    At the end of inflation, this scalar field is usually assumed to decay,
    eventually leaving the universe filled with the standard components of $\lcdm$,
    in thermal equilibrium. The mechanism for this process, known as reheating, is unknown.


Consider an inflaton action of the form
\begin{align}\label{inflaton_action}
S = \int d^4x \sqrt{-g}P(X,\phi)
\end{align}
with $\phi=\phi(t)$ and $X\equiv\frac{1}{2}\dphi^2$.
We will work with the number of e-folds, $N$, as our time variable,
defined by $\frac{a}{a_0}\equiv e^{N-N_0}$.
We then have
$x'\equiv\frac{dx}{dN}=H^{-1}\frac{dx}{dt}$.


The energy-momentum tensor is defined as
\begin{align}\label{gr_energy}
    T_{\mu\nu} = -2\frac{\partial\mathcal{L}_m}{\partial g^{\mu\nu}}+g_{\mu\nu}\mathcal{L}_m
\end{align}
where $\mathcal{L}_m$ is the non-gravitational part of the Lagrangian
density ${\sqrt{-g}\mathcal{L}=\sqrt{-g}\left(\frac{R}{2}+\mathcal{L}_m\right)}$.
The energy and pressure densities can then be identified as
\begin{align}\label{rho_px}
    \rho &= 2XP,_X-P\\
    p &= P,
\end{align}
so then the continuity equation~\eqref{continuity_equation} implies
\begin{align}\label{rho_deriv}
    \rho' = -6XP,_X.
\end{align}
    The ``sound speed'' for the adiabatic perturbations is defined as~\cite{Christopherson_2009}
    \begin{align}\label{sound_speed_definition}
        c_s^2 = \left. \frac{\partial P}{\partial \rho} \right|_S = \frac{\dot{P}}{\dot{\rho}}
        = \frac{P,_X}{\rho,_X}.
    \end{align}
    When we allow perturbations about the background this will have the interpretation
    of a sound speed of those perturbations.
    Since we also have~\eqref{rho_px}, we see that
    \begin{align}
        c_s^2 = \frac{P,_X}{P_X+2X P_{XX}}.
    \end{align}
    For canonical inflation models, $P(X,\phi)=X-V(\phi)$
    so $P_{XX}=0$ and $c_s=1$.
    For non-canonical models with more complicated kinetic terms,
    the sound speed can deviate from unity.
    One such example we will follow in detail in this work is
    Dirac-Born-Infeld ($\dbi$) inflation~\cite{dbi_in_the_sky}.


    For reasons we will discuss in Section~\ref{pert_evol} we will
    define a quantity $\tau_s$
    in analogy with the usual $\tau$ defined in~\eqref{conformal_time_defn}:
    \begin{align}\label{tausdef}
        \tau_s'&\equiv\frac{c_s}{aH}.
    \end{align}
We also define the standard ``slow-roll'' parameters:
\begin{align}\label{slowrollparams}
    \eps &\equiv -\frac{d\ln H}{dN}	\\
    \eta &\equiv +\frac{d\ln \varepsilon}{dN}\\
    \eps_s &\equiv +\frac{d\ln c_s}{dN}.
\end{align}
From the continuity equation and the Friedmann equation
we find the useful relation
\begin{align}\label{epsilon_phi}
    %&&\rho' &= -3(\rho+P)\\
    %\implies&&6HH' &= -3(2X P,_{X})\\
    %\implies&&-H^2\varepsilon &= -\left(\frac{1}{2}H^2{\phi'}^2\right) P,_{X}\\
    %\implies&&\varepsilon &= \frac{1}{2}{\phi'}^2 P,_{X}.
    \varepsilon &= \frac{1}{2}{\phi'}^2 P,_{X}.
\end{align}
Using~\eqref{rho_deriv}, we see that
the equation of motion for $\phi$ is~\cite{Hu_2011}
\begin{align}\label{phieom}
    \phi''+(3c_s^2-\varepsilon)\phi'+H^{-2}\frac{\rho_\phi}{\rho_X}=0.
\end{align}


We will now briefly discuss the special case of canonical models,
with $P(X,\phi)=X-V(\phi)$.
In this case~\eqref{epsilon_phi} simplifies to
$\varepsilon = \frac{1}{2}{\phi'}^2$.
    We also have
    \begin{align}
        \rho_\phi &= \frac{1}{2}\dot{\phi}^2+V(\phi)\\
        P_\phi &= \frac{1}{2}\dot{\phi}^2-V(\phi)\\
        \implies H^2 &= \frac{V(\phi)}{3-\varepsilon}
    \end{align}
    where the last equality used the Friedmann~\eqref{friedmann_1} equation and
    has not assumed $\varepsilon$ is small.
    For successful inflation with the potential of the scalar field acting as a
    cosmological constant, we want $w\approx-1$,
    i.e.\ that $V(\phi)\gg\frac{1}{2}\dot{\phi}^2=\varepsilon H^2$. Since the kinetic term is required to
    be small, this is referred to as \textit{slow-roll} inflation.
    For a canonical model in slow-roll,
    the Friedmann equations then become
    \begin{align}
        H^2 &\approx \frac{1}{3}V(\phi),\\
        \frac{\ddot{a}}{a} &\approx -\frac{1}{6}\left(-2V(\phi)\right).
    \end{align}
    For a model with a canonical kinetic term,~\eqref{phieom} simplifies to
    \begin{align}
        \phi''+(3-\varepsilon)\phi'+H^{-2}V_{\phi}(\phi)=0.
    \end{align}
    In the slow-roll approximation we assume
    that $\phi'$ is not just transiently small,
    i.e.\ that $\phi''\ll\phi'$, and so
    \begin{align}
        \phi'\approx\frac{V_{\phi}(\phi)}{3H^2}\approx\frac{V_{\phi}(\phi)}{V(\phi)}.
    \end{align}
    From this, we see that demanding that $\varepsilon$ be small places a constraint
    on the flatness of the potential in a canonical model.
    Requiring inflation to last sufficiently long
    also constrains $\eta\ll1$.
    %Returning to $P(X,\phi)$,
    %we can see that requiring $\rho_\phi=-p_\phi$
    %using~\eqref{rho_px} demands that $2XP_{,X}\ll P$.


    For inflation to solve the horizon problem, it must result in a shrinking comoving
    Hubble radius, disconnecting regions that had previously been in thermal contact.
    This implies that
    \begin{align}
        \frac{d}{dt}\left(aH\right)^{-1} = -\frac{\ddot{a}}{\dot{a}^2} < 0
    \end{align}
    so $\ddot{a}>0$, i.e.\ the expansion is accelerating. Another way of writing this is
    \begin{align}
        \frac{d}{dt}\left(aH\right)^{-1} &= -\frac{1}{(aH)^2}\left(\dot{a}H+a\dot{H}\right)\\
            &= -\frac{1}{a}\left(1-\varepsilon\right)
    \end{align}
    and so we need $\varepsilon<1$ to successfully inflate the universe.
    However we see that for near-exponential expansion we have already demanded
    $\varepsilon\equiv-\dot{H}/H^2\ll1$ so this must be satisfied.

\section{Statistical observables}
\subsection{From probabilities to observations}\label{corr_functions}
    The discussion until this point has focused on quantities
    with definite values. This will no longer
    be true when we discuss quantities that have a quantum origin, as
    the prediction of a fundamental quantum theory is a statistical one.
    The true prediction is of a distribution from which our observation will be drawn;
    as such, to link the sky we see to some fundamental theory,
    we must talk in terms of probabilities.


    We can define the expectation value of a functional $F$ of a field configuration $f(x)$ as
    \begin{align}
        \left<F\left[f\right]\right> &= \int \mathcal{D}f~F\left[f\right] P\left[f\right]\label{expectation_value_field}
    \end{align}
    where the functional integral $\int \mathcal{D}f$ over $f$ is over all the field configurations
    that $f(x)$ can take, and $P\left[f\right]$ is the corresponding probability distribution.
    In this thesis, we will be working with the expectation value of
    functions of field configurations, so will be working with quantities
    defined in terms of~\eqref{expectation_value_field}.
    In particular, the quantum mechanical nature of the inflationary
    origin of structure means that such predictions
    of inflationary theories will necessarily be statistical. That is to say, the predictions through
    which we will test these theories will be predictions of expectation
    values of these primordial perturbations.


    The ensemble average~\eqref{expectation_value_field} of
    $f(\mathbf{x})f(\mathbf{x'})$ for some function $f$ is known as the
    two-point correlator, or the two-point function: $\left<f(\mathbf{x})f(\mathbf{x'})\right>$.
    Note the brackets $\left<\ldots\right>$ refer to the expected value over the ensemble,
    over all the possible realisations that could be drawn from the probability
    distribution, and not a spatial average.
    %predicted by the inflation model, weighted by their probability.
    Higher order correlators are the
    expectation values of products the field evaluated at more points.


    There are two assumptions we wish to immediately impose on such functions.
    Those conditions are statistical homogeneity and statistical isotropy,
    the requirement that the correlators are invariant under translations and rotations.
    This does not mean that some realisation of the ensemble is forbidden from
    breaking these symmetries, of course---fields
    are still allowed to be inhomogeneous, and thus have special points such as local extrema.
    The statement of statistical homogeneity is that,
    when averaged over the entire ensemble of possible realisations, these special points
    are equally likely to appear anywhere in space.
    All observations of the universe so far appear to respect these two symmetries of the statistics.


    These conditions on the correlators of $f(\vecx)$ place useful constraints on the correlators of the
    Fourier transform, $f(\veck)$. They demand the form
    \begin{align}\label{spectrum_definitions}
        \left<f(\mathbf{k_1})f(\mathbf{k_2})\right> &= (2\pi)^3\delta^{(3)}(\mathbf{k_1}+\mathbf{k_2})P(k_1),\\
        \left<f(\mathbf{k_1})f(\mathbf{k_2})f(\mathbf{k_3})\right> &= (2\pi)^3\delta^{(3)}(\mathbf{k_1}+\mathbf{k_2}+\mathbf{k_3})B(k_1,k_2,k_3),
    \end{align}
    where $P(k)$ is known as the power spectrum and $B(k_1,k_2,k_3)$ is the bispectrum.
    The delta functions come from statistical homogeneity, while the fact that $P(k)$
    and $B(k_1,k_2,k_3)$ depend only on the magnitudes of the vectors ($k=\left|\veck\right|$) is a result
    of demanding statistical isotropy.
    Note that $\left<f(\mathbf{k_1})f(\mathbf{k_2})f(\mathbf{k_3})\right>$
    has $9$ apparent degrees of freedom, but $B(k_1,k_2,k_3)$ has only three.
    This is due to the delta function
    demanding that the sum of the $\mathbf{k_i}$ vanishes, thus forming a triangle.
    Since the orientation of this triangle
    cannot matter, three parameters will suffice. The conventional choice is
    the magnitudes of the vectors.
    Similar statements can of course be made for higher point correlators.


    Our focus in this thesis will be on the predictions that models of inflation make
    for the bispectrum $B(k_1,k_2,k_3)$.
    This is one characteristic property of 
    the initial conditions for the $\lcdm$ evolution. From
    this, one can calculate statistical properties of the $\cmb$ sky that we see.
    If the bispectrum has the form
    \begin{equation}\label{sepXYZ}
        B(k_1,k_2,k_3) = X(k_1)Y(k_2)Z(k_3),
    \end{equation}
    or can be expressed as a sum of such terms, it is called separable.
    This property allows useful simplifications to such calculations, that
    will be discussed in Chapter~\ref{chapter:intro_bispectra}.


    Another symmetry one could imagine is statistical scale invariance.
    Scale invariance is the statement that
    \begin{align}\label{scale_inv}
        \left<f(\mathbf{x})f(\mathbf{x'})\right> = \left<f(\lambda\mathbf{x})f(\lambda\mathbf{x'})\right>.
    \end{align}
    One can show that this symmetry demands\footnote{
        Note that this statement only makes sense for scales smaller
        than some infrared scale, $k>k_{IR}$---if this statement is extended to
        the background ($k=0$) then $\left<f(\mathbf{x})f(\mathbf{x'})\right>$
        will diverge.
    }~$P(k)\propto k^{-3}$.
    Motivated by this, it is usual to define the dimensionless power spectrum
    \begin{align}\label{primpowerspec_defn}
        \mathcal{P}(k) = \frac{k^3}{2\pi^2}P(k).
    \end{align}
    % By looking at <f(x)^2>


    Evidence from the cosmological scales we can observe
    suggests that the symmetry~\eqref{scale_inv} is 
    only approximately respected by the probability
    distribution from which our universe was drawn.
    In fact, it has been estimated that $P(k)\propto k^{n_s-4}$, with $n_s=0.9649 \pm 0.0042$
    at $68\%$ CL~\cite{Planck_inflation_2015, Planck_inflation_2018}.
    Thus we can make the statement that
    the odds of a universe like ours being drawn from a
    perfectly scale-invariant probability
    distribution are negligibly small.


%    We will close this section with a brief note on simulations.
%    Using these simulations, for some given probability distribution, we can determine
%    what our observations might look like---even in the presence of
%    complicating factors such as, in the case of observing the harmonics of the temperature fluctuations
%    in the $\cmb$, the presence of our galaxy blocking a significant portion
%    of the sky.
%    For example, if one makes the assumption that the amplitudes of
%    such harmonics were drawn from a Gaussian distribution with some given power spectrum,
%    one can then generate a large ensemble of possible skies.
%    Processing these by simulating the known instrument effects and
%    sky masks gives an ensemble of realistic experimental data, drawn
%    from a Gaussian distribution. If one then wanted to test the temperature
%    fluctuations of the $\cmb$ for deviations from Gaussianity,
%    for example, one could use this ensemble to estimate the rate of false positives
%    of that test.
%    Thus, simulations provide a way to generate possible observations,
%    even in complicated situations,
%    when the probability distribution is assumed.


    %One such statistical property is the two-point correlation of the temperature $\phi$
    %between two points $x$ and $x'$ in space, $\left<\phi(x)\phi(x')\right>$.
    %We will want to measure these correlations. How can we do that with only access to one realisation
    %of the distribution? It is useful to think about the ergodic theorem. Since an integral over
    %space can be identified with the ensemble average, we can measure, say, the average temperature
    %across space, which should give us an estimate for the average temperature of the ensemble
    %(the quantity that will actually be predicted from theory).
    %Is this related to large cosmic variance at large scales? See pg 94 of Lyth and Liddle.

\subsection{From observations to probabilities}
    Instead of describing observations generated from known probabilities,
    we usually wish to do the opposite.
    We wish to take a series of observations of some random variable and use these observations
    to understand the probability distribution that they were drawn from.
    %Correlation functions are expectation values over ensembles of possible
    %universes, and characterise such probability distributions.
    It is reasonable to wonder how we could ever make contact with a theory
    that only makes these kind of predictions, given that we only have access to one universe.
    Are we doomed from the start? Thankfully, it turns out, that is not the case.


    Inflation is the theory we use to define the probability distribution
    that our universe may have been drawn from.
    %If we only have a finite amount of data from this ensemble,
    %we cannot perfectly measure some given average, as we will be limited by cosmic variance.
    While we may not have access to other universes drawn from the same distribution,
    one can ask how much information we can glean from the one we do have.
    For example, if we magically had access to the entirety of one spatially
    infinite universe then what would our measured spatial averages tell us?
    Would those averages match the ensemble averages?
    If the distribution is such that the correlations between the field evaluated at distant
    points goes to zero sufficiently fast, then we could intuitively imagine that the answer is
    yes---since distant regions are uncorrelated, they are effectively separate draws, and
    so a spatially infinite universe will give us enough information to measure the true
    ensemble average. This intuition is made precise by the ergodic theorem
    (see for example appendix D of~\cite{Weinberg_cosmo}).
    If the assumptions of the ergodic theorem are met,
    then spatial averages and ensemble averages
    match---despite the fact we only have one universe,
    if we had the whole spatial infinity of that universe
    we would still be able to measure the ensemble average.
    Of course, we do not have access to spatial infinity,
    thus our spatial averages will only ever be an approximation of the
    true ensemble averages---this is known as cosmic variance.
    To summarise,
    there are two ways to beat this cosmic variance (to measure an ensemble
    average $\left<f(\vecx)^2\right>$ at some fixed $\vecx$, say).
    We could stay in the same spatial region, but magically peek into all the other universes.
    Or, we could stay in the same universe, but magically peek into every spatial region.
    The ergodic theorem tells us that these two methods will give the same answer,
    thus allowing us to make contact between spatial measurements made in
    our one universe and the ensemble predictions of inflation.


    In practice,
    even in a limited spatial region we can have access to multiple
    samples drawn independently from one distribution. One such situation
    we will see in Section~\ref{sec:power_spec_estimator},
    with the multipole moments of the $\cmb$ temperature fluctuations.
    For a given large angular scale there will be multiple coefficients
    drawn from the same distribution due to statistical isotropy,
    improving the precision with which the parameters of the
    distribution can be estimated.


    %Our result will be a constraint on an inflationary parameter, stated at e.g.\ $95\%$ confidence.
    %For example, if we flipped one fair coin $100$ times, we would expect an even split of heads
    %and tails. Should we be suspicious of the fairness of the coin if we instead get $60$ heads
    %and $40$ tails? Or $80$ heads and $20$ tails? An event at least as extreme at a $60$-$40$ split
    %will occur $3.5\%$ of the time. For a $80$-$20$ split however, the probability of an event that
    %extreme is around one part in $10^{10}$, effectively impossible. This means that if we observed
    %a trial where a coin came up heads in $80$ out of $100$ cases, we could suspect that out hypothesis
    %of fairness was incorrect.
    %One can calculate that, for a fair coin, $95\%$ of the time the number of heads will take a value
    %between $41$ and $59$. We call this the $95\%$ confidence interval.
    %What we wish to do is the opposite, however. Instead of calculating using known probabilities,
    %we wish to take a series of observations of some random variable and use these observations
    %to estimate the probability distribution that they were drawn from. Given an infinite number
    %of observations this task has a straightforward solution, simply plotting the normalised
    %histogram of the results. However, in the context of cosmology we will only have a finite amount
    %of information available to us, and thus a limit to how well we can ever measure the relevant
    %probability distributions. This limitation is known as cosmic variance.



    %We quantify this expected scatter around the mean using a quantity known as the standard deviation.
    %The standard deviation of a random variable $X$ is the square root of the expected value of the
    %squared deviation from the mean $\mu$,
    %\begin{align}
    %    \sigma = \sqrt{E\left[{(X-\mu)}^2\right]}.
    %\end{align}
    %In our example above of flipping one coin $100$ times, the total number of heads has $\mu=50$
    %and $\sigma=5$.

    
    %This is related to the concept
    %of ergodicity. This is the statement that $\left<\cdot\right>$ as an expectation over ensembles
    %at a fixed point is the same as the expectation over points for a fixed ensemble.
    %This assumes homogeneity, stationarity, and that distant points are uncorrelated.
    %In terms of our coin example, this is related to the statement that flipping one coin
    %$100$ times should probe the probability distribution in the same way as flipping
    %$100$ identical coins once each.


    %\begin{align}
    %    \left<\hat{v}_{\bk}\hat{v}_{\bk'}\right>
    %                     &= \left<0|\hat{v}_{\bk}\hat{v}_{\bk'}|0\right>\\
    %                     &= |v_k|^2\left<0\left|\left[\hat{a}_\bk,\hat{a}_{-\bk}^{\dagger}\right]\right|0\right>\\
    %                     &= |v_k|^2\delta(\bk+\bk')\\
    %                     &= P_{v}(k)\delta(\bk+\bk')
    %\end{align}


\section{Thesis outline}
Given the context we have outlined,
    we will now give a description of the goals, methods
    and results presented in this thesis.
    The main goal discussed here is to develop an efficient numerical pipeline for
    connecting inflation models directly to observations through the $\cmb$ bispectrum.
    The concrete results of such a goal are constraints on the parameters of inflation models,
    not constraints on phenomenological templates or summary $\fnl$ parameters.
    This allows the use of the full bispectrum shape information from an inflation scenario,
    not point samples or a limit. The novelty of our results comes from our separable
    approach and numerical methods
    granting access to more accurate, and new, bispectrum shapes.


    To achieve these goals we have developed methods to preserve the
    separability that is built in to the tree-level in-in formalism---these
    terms will be defined in Chapter~\ref{chapter:intro_bispectra}.
    We do this through an expansion in a primordial basis.
    This will link scenarios of inflation to the $\cmb$,
    through an estimator that will be presented in~\cite{Sohn_2021}.
    The calculations involved in that estimator are expensive,
    but need only be done once per primordial basis.
    This motivates the desire for a basis that converges quickly for a broad range of inflation models,
    and as such we have explored this topic in detail.


The first line of research presented in this thesis
is the determination of the feasibility of the overall method.
The initial result here was identifying the contributions of the non-physical configurations
as a novel problem to this formalism,
and identifying basis choice as the key method of overcoming this difficulty.
In this work we describe multiple basis sets and make quantitative comparisons
of their convergence on realistic and interesting models, including ones with features.
These basis sets can overcome the difficulty of the dominant non-physical $k$-configurations,
converging far more efficiently than the basic basis sets, for physically interesting models.


The second line of research presented is the development of
basis-independent methods that allow the fast and accurate calculation
of higher order coefficients of the basis expansion of the tree-level in-in formalism.
To this end we set up the separable formalism and describe the methods used to overcome the
difficulties encountered.
These difficulties include accurately including the highly oscillatory early-time contributions,
and avoiding difficult numerical cancellations between terms in the interaction Hamiltonian
(and corrections from field redefinitions).
A careful and comprehensive validation of our methods is presented,
showing that the convergence problems can be overcome, the oscillatory contributions captured efficiently.
At the primordial level this is done by validating our results using three distinct tests.
Validation is done on established templates from the literature with non-trivial features;
the squeezed-limit consistency condition is verified; our
full bispectrum results are compared to previous codes through point-tests.
%This is the first development of a formalism
%for calculating the separable expansion to sufficiently
%high orders as to describe inflationary features.


As a side-effect of this desire for a separable primordial bispectrum,
the primordial calculation of the inflationary bispectrum presented in this work is much more efficient
than previous methods,
in the sense that it converges far faster in modes than previous methods would in point samples.
These methods are implemented in the $\primodal$ code,
a Python code which can quickly and precisely produce a full feature bispectrum,
despite minimal optimisation. It is presently implemented for single-field models
of inflation.


    The final highlighted result, obtained in collaboration, is the connection
    of an inflationary scenario to the $\cmb$ through the $\cmbbest$ code\footnote{
        Developed by Wuhyun Sohn.}.
    This allowed in particular a constraint to be placed on the sound speed of $\dbi$ inflation, $c_s$.
    At the level of the CMB, validation of the method as a whole
    (when applied to the $\planck$ data)
    is achieved by comparison with the
    $\planck$ constraint on the $\dbi$ sound speed.
    This constraint is translated into a constraint on
    a fundamental parameter in the context of the inflationary parameter scan.


\bigskip


\textbf{Chapter~\ref{chapter:intro_bispectra}} of this thesis
is an introduction to the bispectrum as an inflationary observable.
The various parts of the pipeline that connects inflation scenarios to observations
through the bispectrum are reviewed.
The usual paradigm of bispectrum estimation in the CMB is outlined,
along with the motivation for separable bispectra. The in-in formalism
for calculating the tree level bispectrum for a given model of inflation
is presented in detail, and the point at which the separable formalism diverges is
highlighted.
Reviews of $P(X,\phi)$ models of inflation, and
%some of the usual approximate bispectrum templates
%that we aim to bypass.
%We will draw our validation scenarios from these models.
discussions of previous numerical codes for
calculating the primordial bispectrum $k$-configuration by $k$-configuration
are presented.
%which contrasts our separable basis expansion.
Also reviewed is the previous work in achieving separability through modal expansions
in~\cite{Funakoshi}.
%and we discuss methods of testing
%numerical bispectrum results, defining our relative difference measurement.



\textbf{Chapter~\ref{chapter:decomp}} describes the first line of research mentioned above---the
discussion covers the need to
take the non-physical configurations into account and how the basis sets
presented in this work are built.
Also presented is a quantitative comparison of the convergence of each basis set to
relevant examples of bispectrum shapes.


%By recasting the usual in-in calculation into an explicitly separable form,
%in terms of an expansion in an arbitrary basis.
Since the paradigm presented here is only viable if one can find a basis
that can efficiently represent a wide variety of bispectra,
the majority of Chapter~\ref{chapter:decomp} is devoted to presenting this exploration.
This topic is distinct and separate from our numerical methods, but nonetheless vital.
%Attention is given to the effects of the
%non-physical $k$-configurations on the convergence of our expansion on
%the tetrapyd, and multiple efficient basis sets are presented.



\textbf{Chapter~\ref{chapter:methods}} details the second line of research mentioned above---a
discussion of the details of recasting the in-in formalism in an explicitly separable form is presented,
making explicit each step of the calculation.
Methods for efficiently dealing with the early time contributions to the integrals are discussed,
as well as other numerical issues that require care and attention.
Also presented are validation tests on a very broad range of types of non-Gaussianity.


%We present the precise methods involved in the
%separable formalism and its numerical implementation,
%and detail our methods for carefully calculating the coefficients to high order.
%In particular in Section~\ref{sec:validation} we validate our methods and implementation
%on inflation scenarios with varied features from the literature.


\textbf{Chapter~\ref{chapter:constraints}} presents a constraint
on the sound speed of the $\dbi$ model, which validates our pipeline against a constraint
from the $\planck$ analysis.
This chapter will describe the parametrisation of the scan,
and pay careful attention to the convergence of the result
with respect to the basis size.


\textbf{Chapter~\ref{chapter:conclusion}} discusses possible avenues of future work,
and presents our conclusions.

